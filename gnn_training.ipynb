{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ff7f4e",
   "metadata": {},
   "source": [
    "# Toxic Comment Heterogeneous GNN Training Notebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4b46f",
   "metadata": {},
   "source": [
    "## 1. Load Context Configuration (Parse context.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97991e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG =>\n",
      "{\n",
      "  \"data_csv\": \"Notebooks/youtube_comments_with_toxicity_20250914_061551.csv\",\n",
      "  \"user_col\": \"AuthorChannelID\",\n",
      "  \"comment_col\": \"CommentText\",\n",
      "  \"comment_id_col\": \"CommentID\",\n",
      "  \"parent_col\": \"ParentCommentID\",\n",
      "  \"label_col\": \"ToxicLabel\",\n",
      "  \"score_col\": \"ToxicScore\",\n",
      "  \"binary_label_col\": \"ToxicBinary\",\n",
      "  \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "  \"gnn_hidden\": 128,\n",
      "  \"gnn_out_classes\": 2,\n",
      "  \"gnn_dropout\": 0.3,\n",
      "  \"gnn_num_layers\": 2,\n",
      "  \"use_class_weights\": true,\n",
      "  \"focal_loss\": false,\n",
      "  \"focal_gamma\": 2.0,\n",
      "  \"add_knn_similarity\": true,\n",
      "  \"knn_k\": 5,\n",
      "  \"knn_max_nodes\": 50000,\n",
      "  \"train_val_test_split\": [\n",
      "    0.8,\n",
      "    0.1,\n",
      "    0.1\n",
      "  ],\n",
      "  \"primary_metric\": \"f1\",\n",
      "  \"epochs\": 30,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"early_stopping_patience\": 5,\n",
      "  \"batch_size_node_loader\": 1024,\n",
      "  \"neighbors\": 10,\n",
      "  \"community_min_degree\": 2,\n",
      "  \"community_max_nodes\": 100000,\n",
      "  \"motif_max_nodes\": 200000,\n",
      "  \"edge_classifier_epochs\": 20,\n",
      "  \"edge_classifier_balance\": true,\n",
      "  \"seed\": 42\n",
      "}\n",
      "Loaded dataframe shape: (1032225, 14)\n",
      "Loaded dataframe shape: (1032225, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentID</th>\n",
       "      <th>VideoID</th>\n",
       "      <th>VideoTitle</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>AuthorChannelID</th>\n",
       "      <th>CommentText</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Replies</th>\n",
       "      <th>PublishedAt</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>ToxicLabel</th>\n",
       "      <th>ToxicScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UgyRjrEdJIPrf68uND14AaABAg</td>\n",
       "      <td>mcY4M9gjtsI</td>\n",
       "      <td>They killed my friend.#tales #movie #shorts</td>\n",
       "      <td>@OneWhoWandered</td>\n",
       "      <td>UC_-UEXaBL1dqqUPGkDll49A</td>\n",
       "      <td>Anyone know what movie this is?</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-15 00:54:55</td>\n",
       "      <td>NZ</td>\n",
       "      <td>1</td>\n",
       "      <td>non-toxic</td>\n",
       "      <td>0.998745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UgxXxEIySAwnMNw8D7N4AaABAg</td>\n",
       "      <td>2vuXcw9SZbA</td>\n",
       "      <td>Man Utd conceding first penalty at home in yea...</td>\n",
       "      <td>@chiefvon3068</td>\n",
       "      <td>UCZ1LcZESjYqzaQRhjdZJFwg</td>\n",
       "      <td>The fact they're holding each other back while...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-13 23:51:46</td>\n",
       "      <td>AU</td>\n",
       "      <td>17</td>\n",
       "      <td>non-toxic</td>\n",
       "      <td>0.996063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    CommentID      VideoID  \\\n",
       "0  UgyRjrEdJIPrf68uND14AaABAg  mcY4M9gjtsI   \n",
       "1  UgxXxEIySAwnMNw8D7N4AaABAg  2vuXcw9SZbA   \n",
       "\n",
       "                                          VideoTitle       AuthorName  \\\n",
       "0        They killed my friend.#tales #movie #shorts  @OneWhoWandered   \n",
       "1  Man Utd conceding first penalty at home in yea...    @chiefvon3068   \n",
       "\n",
       "            AuthorChannelID  \\\n",
       "0  UC_-UEXaBL1dqqUPGkDll49A   \n",
       "1  UCZ1LcZESjYqzaQRhjdZJFwg   \n",
       "\n",
       "                                         CommentText Sentiment  Likes  \\\n",
       "0                    Anyone know what movie this is?   Neutral      0   \n",
       "1  The fact they're holding each other back while...  Positive      0   \n",
       "\n",
       "   Replies          PublishedAt CountryCode  CategoryID ToxicLabel  ToxicScore  \n",
       "0        2  2025-01-15 00:54:55          NZ           1  non-toxic    0.998745  \n",
       "1        0  2025-01-13 23:51:46          AU          17  non-toxic    0.996063  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, json, os, textwrap\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "CONTEXT_FILE = Path('context.md')\n",
    "assert CONTEXT_FILE.exists(), 'context.md not found in workspace.'\n",
    "raw = CONTEXT_FILE.read_text(encoding='utf-8')\n",
    "\n",
    "# heuristic extraction of CSV path\n",
    "m = re.search(r'youtube_comments_with_toxicity_(\\d+_\\d+)\\.csv', raw)\n",
    "if m:\n",
    "    csv_candidates = list(Path('Notebooks').glob(f'youtube_comments_with_toxicity_{m.group(1)}.csv'))\n",
    "else:\n",
    "    csv_candidates = list(Path('Notebooks').glob('youtube_comments_with_toxicity_*.csv'))\n",
    "DATA_CSV = csv_candidates[0] if csv_candidates else None\n",
    "\n",
    "config = {\n",
    "    'data_csv': str(DATA_CSV) if DATA_CSV else None,\n",
    "    'user_col': 'AuthorChannelID',\n",
    "    'comment_col': 'CommentText',\n",
    "    'comment_id_col': 'CommentID',\n",
    "    'parent_col': 'ParentCommentID',\n",
    "    'label_col': 'ToxicLabel',\n",
    "    'score_col': 'ToxicScore',\n",
    "    'binary_label_col': 'ToxicBinary',\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',\n",
    "    'gnn_hidden': 128,\n",
    "    'gnn_out_classes': 2,\n",
    "    'gnn_dropout': 0.3,\n",
    "    'gnn_num_layers': 2,\n",
    "    'use_class_weights': True,\n",
    "    'focal_loss': False,          # set True to experiment later\n",
    "    'focal_gamma': 2.0,\n",
    "    'add_knn_similarity': True,\n",
    "    'knn_k': 5,\n",
    "    'knn_max_nodes': 50000,       # cap for kNN (sampled subset when many comments)\n",
    "    'train_val_test_split': [0.8,0.1,0.1],\n",
    "    'primary_metric': 'f1',\n",
    "    'epochs': 30,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'early_stopping_patience': 5,\n",
    "    'batch_size_node_loader': 1024,\n",
    "    'neighbors': 10,\n",
    "    'community_min_degree': 2,    # filter users below this degree before community detection\n",
    "    'community_max_nodes': 100000, # skip / downsample if exceeds\n",
    "    'motif_max_nodes': 200000,    # skip detailed motif / triad census if bigger\n",
    "    'edge_classifier_epochs': 20,\n",
    "    'edge_classifier_balance': True,\n",
    "    'seed': 42\n",
    "}\n",
    "print('CONFIG =>')\n",
    "print(json.dumps(config, indent=2))\n",
    "\n",
    "if not DATA_CSV:\n",
    "    raise FileNotFoundError('Could not locate toxicity CSV. Please place it under Notebooks/.')\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "print('Loaded dataframe shape:', df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f0a3c",
   "metadata": {},
   "source": [
    "## 2. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124a9a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version OK\n",
      "Torch: 2.8.0\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch, random\n",
    "from pathlib import Path\n",
    "\n",
    "REQUIRED = ['pandas','numpy','torch','sklearn','tqdm']\n",
    "print('Python version OK')\n",
    "print('Torch:', torch.__version__)\n",
    "\n",
    "def set_seed(seed:int):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(config.get('seed', 42))\n",
    "\n",
    "device = torch.device('cpu')  # Force CPU to avoid MPS OOM\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1cc3b",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66cabde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 1032225\n",
      "Class balance: ToxicBinary\n",
      "0    0.953549\n",
      "1    0.046451\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentID</th>\n",
       "      <th>VideoID</th>\n",
       "      <th>VideoTitle</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>AuthorChannelID</th>\n",
       "      <th>CommentText</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Replies</th>\n",
       "      <th>PublishedAt</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>ToxicLabel</th>\n",
       "      <th>ToxicScore</th>\n",
       "      <th>ToxicBinary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UgyRjrEdJIPrf68uND14AaABAg</td>\n",
       "      <td>mcY4M9gjtsI</td>\n",
       "      <td>They killed my friend.#tales #movie #shorts</td>\n",
       "      <td>@OneWhoWandered</td>\n",
       "      <td>UC_-UEXaBL1dqqUPGkDll49A</td>\n",
       "      <td>Anyone know what movie this is?</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-15 00:54:55</td>\n",
       "      <td>NZ</td>\n",
       "      <td>1</td>\n",
       "      <td>non-toxic</td>\n",
       "      <td>0.998745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UgxXxEIySAwnMNw8D7N4AaABAg</td>\n",
       "      <td>2vuXcw9SZbA</td>\n",
       "      <td>Man Utd conceding first penalty at home in yea...</td>\n",
       "      <td>@chiefvon3068</td>\n",
       "      <td>UCZ1LcZESjYqzaQRhjdZJFwg</td>\n",
       "      <td>The fact they're holding each other back while...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-13 23:51:46</td>\n",
       "      <td>AU</td>\n",
       "      <td>17</td>\n",
       "      <td>non-toxic</td>\n",
       "      <td>0.996063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UgxB0jh2Ur41mcXr5IB4AaABAg</td>\n",
       "      <td>papg2tsoFzg</td>\n",
       "      <td>Welcome to Javascript Course</td>\n",
       "      <td>@Abdulla-ip8qr</td>\n",
       "      <td>UCWBK35w5Swy1iF5xIbEyw3A</td>\n",
       "      <td>waiting next video will be?</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-07-06 13:18:16</td>\n",
       "      <td>IN</td>\n",
       "      <td>27</td>\n",
       "      <td>non-toxic</td>\n",
       "      <td>0.997976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    CommentID      VideoID  \\\n",
       "0  UgyRjrEdJIPrf68uND14AaABAg  mcY4M9gjtsI   \n",
       "1  UgxXxEIySAwnMNw8D7N4AaABAg  2vuXcw9SZbA   \n",
       "2  UgxB0jh2Ur41mcXr5IB4AaABAg  papg2tsoFzg   \n",
       "\n",
       "                                          VideoTitle       AuthorName  \\\n",
       "0        They killed my friend.#tales #movie #shorts  @OneWhoWandered   \n",
       "1  Man Utd conceding first penalty at home in yea...    @chiefvon3068   \n",
       "2                       Welcome to Javascript Course   @Abdulla-ip8qr   \n",
       "\n",
       "            AuthorChannelID  \\\n",
       "0  UC_-UEXaBL1dqqUPGkDll49A   \n",
       "1  UCZ1LcZESjYqzaQRhjdZJFwg   \n",
       "2  UCWBK35w5Swy1iF5xIbEyw3A   \n",
       "\n",
       "                                         CommentText Sentiment  Likes  \\\n",
       "0                    Anyone know what movie this is?   Neutral      0   \n",
       "1  The fact they're holding each other back while...  Positive      0   \n",
       "2                        waiting next video will be?   Neutral      1   \n",
       "\n",
       "   Replies          PublishedAt CountryCode  CategoryID ToxicLabel  \\\n",
       "0        2  2025-01-15 00:54:55          NZ           1  non-toxic   \n",
       "1        0  2025-01-13 23:51:46          AU          17  non-toxic   \n",
       "2        0  2020-07-06 13:18:16          IN          27  non-toxic   \n",
       "\n",
       "   ToxicScore  ToxicBinary  \n",
       "0    0.998745            0  \n",
       "1    0.996063            0  \n",
       "2    0.997976            0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure required columns & create binary label\n",
    "USER_COL=config['user_col']; COMMENT_COL=config['comment_col']; CID_COL=config['comment_id_col']\n",
    "if 'ToxicBinary' not in df.columns:\n",
    "    if df['ToxicLabel'].dtype==object:\n",
    "        df['ToxicBinary']=df['ToxicLabel'].str.lower().str.startswith('toxic').astype(int)\n",
    "    else:\n",
    "        df['ToxicBinary']=(df['ToxicScore']>0.7).astype(int)\n",
    "\n",
    "# Drop rows missing essentials\n",
    "df = df.dropna(subset=[USER_COL, COMMENT_COL])\n",
    "\n",
    "print('Rows after cleaning:', len(df))\n",
    "print('Class balance:', df['ToxicBinary'].value_counts(normalize=True))\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ff63c",
   "metadata": {},
   "source": [
    "## 4. Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923d5872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falling back to bag-of-words (hash) embeddings due to error: No module named 'sentence_transformers'\n",
      "Embeddings shape: (1032225, 4)\n",
      "Embeddings shape: (1032225, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model_name = config['embedding_model']\n",
    "    sbert = SentenceTransformer(model_name)\n",
    "    texts = df[COMMENT_COL].astype(str).tolist()\n",
    "    batch=256; embs=[]\n",
    "    for i in range(0,len(texts),batch):\n",
    "        embs.append(sbert.encode(texts[i:i+batch], show_progress_bar=False))\n",
    "    import numpy as np\n",
    "    embeddings = np.vstack(embs)\n",
    "except Exception as e:\n",
    "    print('Falling back to bag-of-words (hash) embeddings due to error:', e)\n",
    "    import numpy as np, hashlib\n",
    "    def hvec(t):\n",
    "        h = hashlib.md5(t.encode()).hexdigest()\n",
    "        return np.array([int(h[i:i+4],16)%10000 for i in range(0,16,4)],dtype=float)\n",
    "    embeddings = np.vstack([hvec(t) for t in df[COMMENT_COL].astype(str)])\n",
    "\n",
    "print('Embeddings shape:', embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a2050",
   "metadata": {},
   "source": [
    "## 5. Build Graph (HeteroData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6d7a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sainavamodak/Desktop/FAFO/social/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added similarity edges: 250257\n",
      "HeteroData(\n",
      "  comment={\n",
      "    x=[1032225, 4],\n",
      "    y=[1032225],\n",
      "  },\n",
      "  user={ x=[759619, 2] },\n",
      "  (user, authored, comment)={ edge_index=[2, 1032225] },\n",
      "  (comment, replies_to, comment)={ edge_index=[2, 1027662] },\n",
      "  (comment, similar, comment)={ edge_index=[2, 250257] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch, numpy as np\n",
    "\n",
    "# Reply edges heuristic if parent col missing (normalize IDs to str)\n",
    "if config['parent_col'] in df.columns and df[config['parent_col']].notna().any():\n",
    "    reply_pairs = (\n",
    "        df[df[config['parent_col']].notna()][[config['parent_col'], config['comment_id_col']]]\n",
    "        .astype(str)\n",
    "        .values\n",
    "        .tolist()\n",
    "    )\n",
    "else:\n",
    "    reply_pairs = []\n",
    "    if 'VideoID' in df.columns and 'PublishedAt' in df.columns:\n",
    "        df_sorted = df.sort_values(['VideoID','PublishedAt'])\n",
    "        for vid, group in df_sorted.groupby('VideoID'):\n",
    "            ids = group[config['comment_id_col']].astype(str).tolist()\n",
    "            for i in range(1, len(ids)):\n",
    "                reply_pairs.append((ids[i-1], ids[i]))\n",
    "\n",
    "# Canonicalize IDs to strings for consistent indexing\n",
    "comment_ids = df[CID_COL].astype(str).tolist()\n",
    "comment_idx = {cid: i for i, cid in enumerate(comment_ids)}\n",
    "users = df[USER_COL].astype(str).unique().tolist()\n",
    "user_idx = {u: i for i, u in enumerate(users)}\n",
    "\n",
    "# user->comment authored edges (normalize to str)\n",
    "authored_edges = [(str(row[USER_COL]), str(row[CID_COL])) for _, row in df.iterrows()]\n",
    "\n",
    "# Build features for users\n",
    "user_deg = {u: 0 for u in users}; user_tox = {u: [] for u in users}\n",
    "for _, r in df.iterrows():\n",
    "    u = str(r[USER_COL]); user_deg[u] += 1; user_tox[u].append(r['ToxicBinary'])\n",
    "user_feat = np.vstack([\n",
    "    [user_deg[u] for u in users],\n",
    "    [np.mean(user_tox[u]) if user_tox[u] else 0 for u in users]\n",
    "]).T\n",
    "\n",
    "hetero = HeteroData()\n",
    "hetero['comment'].x = torch.tensor(embeddings, dtype=torch.float)\n",
    "hetero['comment'].y = torch.tensor(df['ToxicBinary'].values, dtype=torch.long)\n",
    "hetero['user'].x = torch.tensor(user_feat, dtype=torch.float)\n",
    "\n",
    "# Build edge indices (guard for any stray IDs)\n",
    "src = [user_idx[str(u)] for u, c in authored_edges if str(u) in user_idx and str(c) in comment_idx]\n",
    "dst = [comment_idx[str(c)] for u, c in authored_edges if str(u) in user_idx and str(c) in comment_idx]\n",
    "hetero['user','authored','comment'].edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "r_src = [comment_idx[str(p)] for p, c in reply_pairs if str(p) in comment_idx and str(c) in comment_idx]\n",
    "r_dst = [comment_idx[str(c)] for p, c in reply_pairs if str(p) in comment_idx and str(c) in comment_idx]\n",
    "hetero['comment','replies_to','comment'].edge_index = torch.tensor([r_src, r_dst], dtype=torch.long)\n",
    "\n",
    "# Optional: add kNN similarity edges among comments to densify the graph\n",
    "if config.get('add_knn_similarity', False):\n",
    "    max_nodes = config.get('knn_max_nodes', 50000)\n",
    "    emb_tensor = hetero['comment'].x\n",
    "    total_nodes = emb_tensor.size(0)\n",
    "    if total_nodes > 10:  # only if meaningful\n",
    "        if total_nodes > max_nodes:\n",
    "            # sample a subset for similarity graph; map back indices\n",
    "            sample_idx = torch.randperm(total_nodes)[:max_nodes]\n",
    "            emb_sample = emb_tensor[sample_idx]\n",
    "            base_indices = sample_idx\n",
    "        else:\n",
    "            emb_sample = emb_tensor\n",
    "            base_indices = torch.arange(total_nodes)\n",
    "        # Normalize and compute approximate cosine similarity via inner product\n",
    "        with torch.no_grad():\n",
    "            normed = torch.nn.functional.normalize(emb_sample, p=2, dim=1)\n",
    "            # chunked to control memory\n",
    "            k = config.get('knn_k', 5)\n",
    "            edges_sim_src = []\n",
    "            edges_sim_dst = []\n",
    "            chunk = 2048\n",
    "            for start in range(0, normed.size(0), chunk):\n",
    "                blk = normed[start:start+chunk]\n",
    "                sim = blk @ normed.T  # [chunk, N]\n",
    "                topk = torch.topk(sim, k=k+1, dim=1).indices  # include self then filter\n",
    "                base_rows = base_indices[start:start+chunk]\n",
    "                for row_i, neighs in zip(base_rows.tolist(), topk):\n",
    "                    for n in neighs.tolist():\n",
    "                        if base_indices[n] != row_i:  # skip self\n",
    "                            edges_sim_src.append(row_i)\n",
    "                            edges_sim_dst.append(base_indices[n].item())\n",
    "            if edges_sim_src:\n",
    "                hetero['comment','similar','comment'].edge_index = torch.tensor([edges_sim_src, edges_sim_dst], dtype=torch.long)\n",
    "                print(f\"Added similarity edges: {len(edges_sim_src)}\")\n",
    "\n",
    "print(hetero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2348144",
   "metadata": {},
   "source": [
    "## 6. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339257af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 825780 103222 103223\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_comments = hetero['comment'].num_nodes\n",
    "perm = torch.randperm(num_comments)\n",
    "n_train=int(config['train_val_test_split'][0]*num_comments)\n",
    "n_val=int(config['train_val_test_split'][1]*num_comments)\n",
    "train_idx=perm[:n_train]; val_idx=perm[n_train:n_train+n_val]; test_idx=perm[n_train+n_val:]\n",
    "train_mask=torch.zeros(num_comments,dtype=torch.bool); train_mask[train_idx]=True\n",
    "val_mask=torch.zeros(num_comments,dtype=torch.bool); val_mask[val_idx]=True\n",
    "test_mask=torch.zeros(num_comments,dtype=torch.bool); test_mask[test_idx]=True\n",
    "hetero['comment'].train_mask=train_mask\n",
    "hetero['comment'].val_mask=val_mask\n",
    "hetero['comment'].test_mask=test_mask\n",
    "print('Split sizes:', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c1100",
   "metadata": {},
   "source": [
    "## 7. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d7c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroGNN(\n",
      "  (in_lins): ModuleDict(\n",
      "    (user): Linear(-1, 128, bias=True)\n",
      "    (comment): Linear(-1, 128, bias=True)\n",
      "  )\n",
      "  (convs): ModuleList(\n",
      "    (0-1): 2 x HeteroConv(num_relations=3)\n",
      "  )\n",
      "  (lin): Linear(128, 2, bias=True)\n",
      ")\n",
      "Model and data both on: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sainavamodak/Desktop/FAFO/social/lib/python3.12/site-packages/torch_geometric/nn/conv/hetero_conv.py:76: UserWarning: There exist node types ({'user'}) whose representations do not get updated during message passing as they do not occur as destination type in any edge type. This may lead to unexpected behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Build metadata-driven multi-layer hetero GNN with dropout\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden, out_classes, num_layers=2, dropout=0.3, edge_types=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.hidden = hidden\n",
    "        self.out_classes = out_classes\n",
    "        rels = set(edge_types or [])\n",
    "        if not rels:\n",
    "            raise ValueError('HeteroGNN requires at least one edge type; got none.')\n",
    "        # Type-wise input projection to hidden dim (lazy Linear infers input dim on first use)\n",
    "        self.in_lins = nn.ModuleDict({\n",
    "            'user': Linear(-1, hidden),\n",
    "            'comment': Linear(-1, hidden),\n",
    "        })\n",
    "        # Helper to build relation dict depending on available relations\n",
    "        def build_rel_dict():\n",
    "            rel_dict = {}\n",
    "            if ('user','authored','comment') in rels:\n",
    "                rel_dict[('user','authored','comment')] = SAGEConv((hidden, hidden), hidden)\n",
    "            if ('comment','replies_to','comment') in rels:\n",
    "                rel_dict[('comment','replies_to','comment')] = SAGEConv((hidden, hidden), hidden)\n",
    "            if ('comment','similar','comment') in rels:\n",
    "                rel_dict[('comment','similar','comment')] = SAGEConv((hidden, hidden), hidden)\n",
    "            return rel_dict\n",
    "        # All layers operate on hidden-sized features\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(HeteroConv(build_rel_dict(), aggr='mean'))\n",
    "        self.lin = Linear(hidden, out_classes)\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Project raw node features to hidden size\n",
    "        x_dict = {k: self.in_lins[k](v) if k in self.in_lins else v for k, v in x_dict.items()}\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            out_dict = conv(x_dict, edge_index_dict)\n",
    "            # Merge: keep previous embeddings for node types not updated in this layer\n",
    "            merged = {}\n",
    "            for k in x_dict.keys():\n",
    "                v = out_dict.get(k, x_dict[k])\n",
    "                merged[k] = F.dropout(v.relu(), p=self.dropout, training=self.training)\n",
    "            x_dict = merged\n",
    "        return self.lin(x_dict['comment'])\n",
    "\n",
    "model = HeteroGNN(\n",
    "    config['gnn_hidden'],\n",
    "    config['gnn_out_classes'],\n",
    "    config['gnn_num_layers'],\n",
    "    config['gnn_dropout'],\n",
    "    edge_types=hetero.edge_types\n",
    ").to(device)\n",
    "# Ensure hetero data is also on the same device as model\n",
    "hetero = hetero.to(device)\n",
    "print(model)\n",
    "print(f'Model and data both on: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4cd8f5",
   "metadata": {},
   "source": [
    "## 8. Training Hyperparameters & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ade09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "\n",
    "# Optional focal loss implementation\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, target):\n",
    "        ce = torch.nn.functional.cross_entropy(logits, target, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1-pt)**self.gamma) * ce\n",
    "        if self.reduction=='mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction=='sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# Class weights for imbalance\n",
    "if config.get('use_class_weights', False):\n",
    "    y_all = hetero['comment'].y.detach().cpu().numpy()\n",
    "    counts = np.bincount(y_all)\n",
    "    total = counts.sum()\n",
    "    weights = total / (len(counts) * counts)\n",
    "    class_w_tensor = torch.tensor(weights, dtype=torch.float, device=device)\n",
    "else:\n",
    "    class_w_tensor = None\n",
    "\n",
    "if config.get('focal_loss', False):\n",
    "    criterion = FocalLoss(gamma=config.get('focal_gamma', 2.0), weight=class_w_tensor)\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_w_tensor)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "best_metric=-1.0\n",
    "patience=0\n",
    "best_path = Path('model_best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2a130",
   "metadata": {},
   "source": [
    "## 9. Training Loop with Validation & Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a0afa",
   "metadata": {},
   "source": [
    "## 8.5. Device Sync and Diagnostic Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f3f1e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Device Sync Check ===\n",
      "Target device: cpu\n",
      "hetero.x_dict devices: ['comment:cpu', 'user:cpu']\n",
      "Model device: cpu\n",
      "Class weights device: cpu\n",
      "Testing forward pass...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sainavamodak/Desktop/FAFO/social/lib/python3.12/site-packages/torch_geometric/nn/conv/hetero_conv.py:76: UserWarning: There exist node types ({'user'}) whose representations do not get updated during message passing as they do not occur as destination type in any edge type. This may lead to unexpected behavior.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Forward pass successful! Output shape: torch.Size([1032225, 2])\n",
      "=== Ready for training ===\n"
     ]
    }
   ],
   "source": [
    "# Force everything to CPU and verify device consistency\n",
    "print(\"=== Device Sync Check ===\")\n",
    "print(f\"Target device: {device}\")\n",
    "\n",
    "# Move hetero graph to CPU (may already be there, but ensure consistency)\n",
    "hetero = hetero.cpu()\n",
    "print(f\"hetero.x_dict devices: {[f'{k}:{v.device}' for k,v in hetero.x_dict.items()]}\")\n",
    "\n",
    "# Recreate model and optimizer on CPU (fresh start)\n",
    "model = HeteroGNN(\n",
    "    config['gnn_hidden'],\n",
    "    config['gnn_out_classes'],\n",
    "    config['gnn_num_layers'],\n",
    "    config['gnn_dropout'],\n",
    "    edge_types=hetero.edge_types\n",
    ").to(device)\n",
    "\n",
    "# Recreate optimizer and criterion for CPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "if config.get('focal_loss', False):\n",
    "    criterion = FocalLoss(gamma=config.get('focal_gamma', 2.0), weight=class_w_tensor.to(device) if class_w_tensor is not None else None)\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_w_tensor.to(device) if class_w_tensor is not None else None)\n",
    "\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Class weights device: {class_w_tensor.device if class_w_tensor is not None else 'None'}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"Testing forward pass...\")\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        test_out = model(hetero.x_dict, hetero.edge_index_dict)\n",
    "        print(f\"✅ Forward pass successful! Output shape: {test_out.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Forward pass failed: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"=== Ready for training ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf4cc2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss 292.2709 | val_f1 0.4851 | val_acc 0.8205\n",
      "  ✅ Saved new best model (F1=0.4851)\n",
      "Epoch 02 | loss 605.9946 | val_f1 0.4878 | val_acc 0.9525\n",
      "  ✅ Saved new best model (F1=0.4878)\n",
      "Epoch 02 | loss 605.9946 | val_f1 0.4878 | val_acc 0.9525\n",
      "  ✅ Saved new best model (F1=0.4878)\n",
      "Epoch 03 | loss 372.6259 | val_f1 0.4503 | val_acc 0.6957\n",
      "Epoch 03 | loss 372.6259 | val_f1 0.4503 | val_acc 0.6957\n",
      "Epoch 04 | loss 557.4712 | val_f1 0.0454 | val_acc 0.0475\n",
      "Epoch 04 | loss 557.4712 | val_f1 0.0454 | val_acc 0.0475\n",
      "Epoch 05 | loss 255.8391 | val_f1 0.0454 | val_acc 0.0475\n",
      "Epoch 05 | loss 255.8391 | val_f1 0.0454 | val_acc 0.0475\n",
      "Epoch 06 | loss 458.1595 | val_f1 0.0454 | val_acc 0.0475\n",
      "Epoch 06 | loss 458.1595 | val_f1 0.0454 | val_acc 0.0475\n",
      "Epoch 07 | loss 216.5446 | val_f1 0.2234 | val_acc 0.2462\n",
      "Early stopping triggered.\n",
      "Epoch 07 | loss 216.5446 | val_f1 0.2234 | val_acc 0.2462\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>292.270905</td>\n",
       "      <td>0.485107</td>\n",
       "      <td>0.820542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>605.994629</td>\n",
       "      <td>0.487834</td>\n",
       "      <td>0.952491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>372.625854</td>\n",
       "      <td>0.450315</td>\n",
       "      <td>0.695724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>557.471191</td>\n",
       "      <td>0.045354</td>\n",
       "      <td>0.047509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>255.839096</td>\n",
       "      <td>0.045354</td>\n",
       "      <td>0.047509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch        loss    val_f1   val_acc\n",
       "0      1  292.270905  0.485107  0.820542\n",
       "1      2  605.994629  0.487834  0.952491\n",
       "2      3  372.625854  0.450315  0.695724\n",
       "3      4  557.471191  0.045354  0.047509\n",
       "4      5  255.839096  0.045354  0.047509"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "history = []\n",
    "for epoch in range(1, config['epochs']+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(hetero.x_dict, hetero.edge_index_dict)\n",
    "    y = hetero['comment'].y\n",
    "    loss = criterion(out[hetero['comment'].train_mask], y[hetero['comment'].train_mask])\n",
    "    loss.backward(); optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_eval = model(hetero.x_dict, hetero.edge_index_dict)\n",
    "        val_logits = logits_eval[hetero['comment'].val_mask]\n",
    "        val_y = y[hetero['comment'].val_mask]\n",
    "        preds = val_logits.argmax(dim=1).cpu().numpy()\n",
    "        val_y_np = val_y.cpu().numpy()\n",
    "        f1 = f1_score(val_y_np, preds, average='macro')\n",
    "        acc = accuracy_score(val_y_np, preds)\n",
    "    history.append({'epoch':epoch,'loss':float(loss.item()),'val_f1':float(f1),'val_acc':float(acc)})\n",
    "    print(f\"Epoch {epoch:02d} | loss {loss.item():.4f} | val_f1 {f1:.4f} | val_acc {acc:.4f}\")\n",
    "\n",
    "    if f1>best_metric:\n",
    "        best_metric=f1; patience=0\n",
    "        torch.save({'model_state': model.state_dict(), 'config': config}, best_path)\n",
    "        print('  ✅ Saved new best model (F1={:.4f})'.format(f1))\n",
    "    else:\n",
    "        patience+=1\n",
    "        if patience>=config['early_stopping_patience']:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "import pandas as _pd\n",
    "hist_df = _pd.DataFrame(history)\n",
    "hist_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08e3b1",
   "metadata": {},
   "source": [
    "## 10. Test Evaluation (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f82008d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1=0.4880 Acc=0.9532\n"
     ]
    }
   ],
   "source": [
    "# Load best and evaluate on test\n",
    "state = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(state['model_state'])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(hetero.x_dict, hetero.edge_index_dict)\n",
    "    test_logits = logits[hetero['comment'].test_mask]\n",
    "    test_y = hetero['comment'].y[hetero['comment'].test_mask]\n",
    "    preds = test_logits.argmax(dim=1).cpu().numpy()\n",
    "    y_true = test_y.cpu().numpy()\n",
    "    test_f1 = f1_score(y_true, preds, average='macro')\n",
    "    test_acc = accuracy_score(y_true, preds)\n",
    "print(f'Test F1={test_f1:.4f} Acc={test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ba274",
   "metadata": {},
   "source": [
    "## 11. Serialize Artifacts (Config + Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd6c352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model SHA256: da3f767d20b6cff63f19bfb4b74005afda95e406228e0cb20f703e5b3fc1d301\n"
     ]
    }
   ],
   "source": [
    "import json, hashlib\n",
    "artifacts_dir=Path('artifacts'); artifacts_dir.mkdir(exist_ok=True)\n",
    "json.dump(config, open(artifacts_dir/'config.json','w'), indent=2)\n",
    "# copy model file\n",
    "import shutil\n",
    "shutil.copy(best_path, artifacts_dir/'model_best.pt')\n",
    "# hash\n",
    "h=hashlib.sha256(open(artifacts_dir/'model_best.pt','rb').read()).hexdigest()\n",
    "print('Model SHA256:', h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c294b71",
   "metadata": {},
   "source": [
    "## 11b. Task A: Reply Edge Classification (Abusive vs Non-Abusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae142fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge balancing applied: pos 47812, sampled neg 143436 -> total 191248\n",
      "Edge Epoch 1 loss 41.5258 test_acc 0.7500\n",
      "Edge Epoch 1 loss 41.5258 test_acc 0.7500\n",
      "Edge Epoch 5 loss 23.4409 test_acc 0.2511\n",
      "Edge Epoch 5 loss 23.4409 test_acc 0.2511\n",
      "Edge Epoch 10 loss 21.0688 test_acc 0.7500\n",
      "Edge Epoch 10 loss 21.0688 test_acc 0.7500\n",
      "Edge Epoch 15 loss 9.1223 test_acc 0.4808\n",
      "Edge Epoch 15 loss 9.1223 test_acc 0.4808\n",
      "Edge Epoch 20 loss 5.2507 test_acc 0.7268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.750     0.954     0.840     28688\n",
      "           1      0.245     0.045     0.075      9562\n",
      "\n",
      "    accuracy                          0.727     38250\n",
      "   macro avg      0.497     0.499     0.458     38250\n",
      "weighted avg      0.624     0.727     0.649     38250\n",
      "\n",
      "Wrote artifacts/edge_clf_report.json\n",
      "Edge Epoch 20 loss 5.2507 test_acc 0.7268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.750     0.954     0.840     28688\n",
      "           1      0.245     0.045     0.075      9562\n",
      "\n",
      "    accuracy                          0.727     38250\n",
      "   macro avg      0.497     0.499     0.458     38250\n",
      "weighted avg      0.624     0.727     0.649     38250\n",
      "\n",
      "Wrote artifacts/edge_clf_report.json\n"
     ]
    }
   ],
   "source": [
    "# Prepare edge dataset: label is child comment toxicity with balancing options\n",
    "import torch, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "\n",
    "edge_src = np.array(r_src)\n",
    "edge_dst = np.array(r_dst)\n",
    "\n",
    "if edge_src.size == 0:\n",
    "    print('No reply edges available; skipping edge classification.')\n",
    "else:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Compute hidden embeddings consistent with model.forward (without final classifier)\n",
    "        x_dict_emb = {k: model.in_lins[k](v) if k in model.in_lins else v for k, v in hetero.x_dict.items()}\n",
    "        for conv in model.convs:\n",
    "            out_dict = conv(x_dict_emb, hetero.edge_index_dict)\n",
    "            x_dict_emb = {k: out_dict.get(k, x_dict_emb[k]).relu() for k in x_dict_emb.keys()}\n",
    "        hidden_comment = x_dict_emb['comment']\n",
    "    feat = hidden_comment.detach().cpu().numpy()\n",
    "\n",
    "    X_edge = np.concatenate([feat[edge_src], feat[edge_dst]], axis=1)\n",
    "    y_edge = hetero['comment'].y.detach().cpu().numpy()[edge_dst]\n",
    "\n",
    "    # Balance edges if configured\n",
    "    if config.get('edge_classifier_balance', True):\n",
    "        idx_pos = np.where(y_edge==1)[0]\n",
    "        idx_neg = np.where(y_edge==0)[0]\n",
    "        if len(idx_pos) > 0 and len(idx_neg) > 0:\n",
    "            neg_sample = np.random.choice(idx_neg, size=min(len(idx_neg), max(len(idx_pos)*3, 1)), replace=False)\n",
    "            keep = np.concatenate([idx_pos, neg_sample])\n",
    "            np.random.shuffle(keep)\n",
    "            X_edge = X_edge[keep]\n",
    "            y_edge = y_edge[keep]\n",
    "            print(f'Edge balancing applied: pos {len(idx_pos)}, sampled neg {len(neg_sample)} -> total {len(keep)}')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_edge, y_edge, test_size=0.2, stratify=y_edge if len(np.unique(y_edge))>1 else None, random_state=42\n",
    "    )\n",
    "\n",
    "    import torch.nn as nn\n",
    "    class EdgeMLP(nn.Module):\n",
    "        def __init__(self, in_dim, hidden=256, dropout=0.3):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
    "                nn.Linear(hidden, hidden//2), nn.ReLU(), nn.Dropout(dropout/2),\n",
    "                nn.Linear(hidden//2, 2)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    in_dim = X_train.shape[1]\n",
    "    clf = EdgeMLP(in_dim).to(device)\n",
    "    opt = torch.optim.Adam(clf.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    lossf = nn.CrossEntropyLoss()\n",
    "\n",
    "    Xtr = torch.tensor(X_train, dtype=torch.float, device=device); ytr = torch.tensor(y_train, dtype=torch.long, device=device)\n",
    "    Xte = torch.tensor(X_test, dtype=torch.float, device=device); yte = torch.tensor(y_test, dtype=torch.long, device=device)\n",
    "\n",
    "    epochs_edge = config.get('edge_classifier_epochs', 20)\n",
    "    for epoch in range(1, epochs_edge+1):\n",
    "        clf.train(); opt.zero_grad(); out = clf(Xtr); loss = lossf(out, ytr); loss.backward(); opt.step()\n",
    "        if epoch%5==0 or epoch==1:\n",
    "            clf.eval(); pred = clf(Xte).argmax(1); acc=(pred==yte).float().mean().item(); print(f'Edge Epoch {epoch} loss {loss.item():.4f} test_acc {acc:.4f}')\n",
    "\n",
    "    clf.eval(); pred = clf(Xte).argmax(1).cpu().numpy()\n",
    "    print(classification_report(y_test, pred, digits=3))\n",
    "\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_test, pred, labels=[0,1], zero_division=0)\n",
    "    edge_report = {\n",
    "        'labels': ['non_abusive','abusive'],\n",
    "        'precision': p.tolist(), 'recall': r.tolist(), 'f1': f1.tolist(), 'support': s.tolist()\n",
    "    }\n",
    "    Path('artifacts').mkdir(exist_ok=True)\n",
    "    json.dump(edge_report, open('artifacts/edge_clf_report.json','w'), indent=2)\n",
    "    print('Wrote artifacts/edge_clf_report.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568897f7",
   "metadata": {},
   "source": [
    "## 11c. Motif Counting and k-core (Gang-up Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e346c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote artifacts/motifs_kcore_report.json\n"
     ]
    }
   ],
   "source": [
    "# Convert to NetworkX and compute motifs + k-core (with safety caps)\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import json, math\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(range(hetero['comment'].num_nodes))\n",
    "if 'comment' in hetero.node_types and ('comment','replies_to','comment') in hetero.edge_types:\n",
    "    eidx = hetero['comment','replies_to','comment'].edge_index.detach().cpu().numpy()\n",
    "    for u,v in zip(eidx[0], eidx[1]):\n",
    "        G.add_edge(int(u), int(v))\n",
    "\n",
    "size_cap = config.get('motif_max_nodes', 200000)\n",
    "large_graph = G.number_of_nodes() > size_cap\n",
    "\n",
    "import numpy as np\n",
    "y_np = hetero['comment'].y.detach().cpu().numpy()\n",
    "abusive_edges = [(u,v) for u,v in G.edges() if y_np[v] == 1]\n",
    "\n",
    "# If graph huge, downsample a subgraph for triadic census\n",
    "def maybe_subgraph(Gd):\n",
    "    if not large_graph:\n",
    "        return Gd, False\n",
    "    # sample nodes with activity bias: pick nodes with out-degree >0 first\n",
    "    deg_nodes = [n for n,d in Gd.out_degree() if d>0]\n",
    "    if len(deg_nodes) < size_cap:\n",
    "        chosen = deg_nodes + [n for n in Gd.nodes() if n not in deg_nodes][:size_cap-len(deg_nodes)]\n",
    "    else:\n",
    "        chosen = deg_nodes[:size_cap]\n",
    "    return Gd.subgraph(chosen).copy(), True\n",
    "\n",
    "G_eval, is_sampled = maybe_subgraph(G)\n",
    "\n",
    "abusive_only = nx.DiGraph()\n",
    "if abusive_edges:\n",
    "    abusive_only.add_nodes_from(G_eval.nodes())\n",
    "    abusive_only.add_edges_from([e for e in abusive_edges if e[0] in G_eval and e[1] in G_eval])\n",
    "else:\n",
    "    abusive_only.add_nodes_from(G_eval.nodes())\n",
    "\n",
    "\n",
    "def graph_summary(Gd: nx.DiGraph, label: str):\n",
    "    if Gd.number_of_nodes() == 0:\n",
    "        return {\n",
    "            'n_nodes': 0, 'n_edges': 0, 'density': 0.0,\n",
    "            'avg_clustering': 0.0, 'triadic_census': {}, 'kcore_max': 0,\n",
    "            'reciprocity': 0.0, 'sampled': is_sampled if label=='all_edges' else False\n",
    "        }\n",
    "    und = Gd.to_undirected()\n",
    "    dens = nx.density(und)\n",
    "    try:\n",
    "        avg_clust = nx.average_clustering(und) if und.number_of_edges() > 0 else 0.0\n",
    "    except Exception:\n",
    "        avg_clust = 0.0\n",
    "    # Triadic census only if small enough (networkx triadic_census is O(n^3) worst-case)\n",
    "    triad = {}\n",
    "    if Gd.number_of_nodes() <= 50000 and Gd.number_of_edges() > 0:\n",
    "        try:\n",
    "            triad = nx.triadic_census(Gd)\n",
    "        except Exception:\n",
    "            tri = sum(nx.triangles(und).values()) // 3 if und.number_of_edges() > 0 else 0\n",
    "            triad = {'triangles': int(tri)}\n",
    "    else:\n",
    "        triad = {'skipped': True}\n",
    "    try:\n",
    "        core_nums = nx.core_number(und) if und.number_of_edges() > 0 else {}\n",
    "        kcore_max = int(max(core_nums.values())) if core_nums else 0\n",
    "    except Exception:\n",
    "        kcore_max = 0\n",
    "    try:\n",
    "        reciprocity = nx.reciprocity(Gd)\n",
    "        reciprocity = float(reciprocity) if reciprocity is not None else 0.0\n",
    "    except Exception:\n",
    "        reciprocity = 0.0\n",
    "    return {\n",
    "        'n_nodes': Gd.number_of_nodes(),\n",
    "        'n_edges': Gd.number_of_edges(),\n",
    "        'density': float(dens),\n",
    "        'avg_clustering': float(avg_clust),\n",
    "        'triadic_census': {k:int(v) if isinstance(v,(int,float)) else v for k,v in triad.items()},\n",
    "        'kcore_max': kcore_max,\n",
    "        'reciprocity': reciprocity,\n",
    "        'sampled': is_sampled if label=='all_edges' else False\n",
    "    }\n",
    "\n",
    "report = {}\n",
    "report['all_edges'] = graph_summary(G_eval, 'all_edges')\n",
    "report['abusive_only'] = graph_summary(abusive_only, 'abusive_only')\n",
    "\n",
    "Path('artifacts').mkdir(exist_ok=True)\n",
    "with open('artifacts/motifs_kcore_report.json','w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print('Wrote artifacts/motifs_kcore_report.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa084cf8",
   "metadata": {},
   "source": [
    "## 12. Task B: User–User Graph, Communities, Polarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cca2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote artifacts/user_partition.csv, user_partition_toxic.csv, communities_report.json\n"
     ]
    }
   ],
   "source": [
    "# Build user-user graph from reply edges; run communities; compute metrics with filtering & caps\n",
    "import networkx as nx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "comment_user = df.set_index(CID_COL)[USER_COL].to_dict()\n",
    "U = nx.DiGraph()\n",
    "U.add_nodes_from(users)\n",
    "for p, c in zip(r_src, r_dst):\n",
    "    if p < len(comment_ids) and c < len(comment_ids):\n",
    "        up = comment_user.get(comment_ids[p]); uc = comment_user.get(comment_ids[c])\n",
    "        if up is None or uc is None or up == uc:\n",
    "            continue\n",
    "        U.add_edge(up, uc)\n",
    "\n",
    "# Degree filter before community detection\n",
    "min_deg = config.get('community_min_degree', 2)\n",
    "if min_deg > 0:\n",
    "    active_nodes = [n for n,d in U.degree() if d >= min_deg]\n",
    "    U_sub = U.subgraph(active_nodes).copy()\n",
    "else:\n",
    "    U_sub = U\n",
    "\n",
    "# Size cap\n",
    "max_nodes_comm = config.get('community_max_nodes', 100000)\n",
    "if U_sub.number_of_nodes() > max_nodes_comm:\n",
    "    # sample nodes with highest degree\n",
    "    deg_sorted = sorted(U_sub.degree(), key=lambda x: x[1], reverse=True)[:max_nodes_comm]\n",
    "    keep = set(n for n,_ in deg_sorted)\n",
    "    U_sub = U_sub.subgraph(keep).copy()\n",
    "    sampled_flag = True\n",
    "else:\n",
    "    sampled_flag = False\n",
    "\n",
    "# Toxic-only edges (based on child comment toxicity)\n",
    "y_np = hetero['comment'].y.detach().cpu().numpy()\n",
    "U_toxic = nx.DiGraph()\n",
    "U_toxic.add_nodes_from(U_sub.nodes())\n",
    "for p, c in zip(r_src, r_dst):\n",
    "    if p < len(comment_ids) and c < len(comment_ids) and y_np[c] == 1:\n",
    "        up = comment_user.get(comment_ids[p]); uc = comment_user.get(comment_ids[c])\n",
    "        if up is None or uc is None or up == uc:\n",
    "            continue\n",
    "        if up in U_sub and uc in U_sub:\n",
    "            U_toxic.add_edge(up, uc)\n",
    "\n",
    "Und = U_sub.to_undirected()\n",
    "Und_t = U_toxic.to_undirected()\n",
    "\n",
    "try:\n",
    "    from networkx.algorithms.community import greedy_modularity_communities\n",
    "    comms = list(greedy_modularity_communities(Und)) if Und.number_of_edges()>0 else []\n",
    "    comms_t = list(greedy_modularity_communities(Und_t)) if Und_t.number_of_edges()>0 else []\n",
    "except Exception:\n",
    "    comms, comms_t = [], []\n",
    "\n",
    "part = {}\n",
    "for i, com in enumerate(comms):\n",
    "    for u in com:\n",
    "        part[u] = i\n",
    "part_t = {}\n",
    "for i, com in enumerate(comms_t):\n",
    "    for u in com:\n",
    "        part_t[u] = i\n",
    "\n",
    "mod = None; mod_t = None\n",
    "try:\n",
    "    from networkx.algorithms.community.quality import modularity\n",
    "    if comms:\n",
    "        mod = float(modularity(Und, comms))\n",
    "    if comms_t:\n",
    "        mod_t = float(modularity(Und_t, comms_t))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def ei_index(Gd: nx.DiGraph, partition: dict):\n",
    "    if Gd.number_of_edges() == 0:\n",
    "        return 0.0\n",
    "    internal = external = 0\n",
    "    for u, v in Gd.edges():\n",
    "        cu = partition.get(u, -1); cv = partition.get(v, -1)\n",
    "        if cu == -1 or cv == -1:\n",
    "            continue\n",
    "        if cu == cv:\n",
    "            internal += 1\n",
    "        else:\n",
    "            external += 1\n",
    "    denom = internal + external\n",
    "    return float((external - internal) / denom) if denom > 0 else 0.0\n",
    "\n",
    "ei = ei_index(U_sub, part)\n",
    "ei_t = ei_index(U_toxic, part_t if part_t else part)\n",
    "\n",
    "Path('artifacts').mkdir(exist_ok=True)\n",
    "# partition export restricted to nodes in filtered graph\n",
    "import pandas as pd\n",
    "part_rows = [(u, part.get(u, -1)) for u in U_sub.nodes()]\n",
    "part_t_rows = [(u, part_t.get(u, -1)) for u in U_sub.nodes()]\n",
    "\n",
    "pd.DataFrame(part_rows, columns=['user','community']).to_csv('artifacts/user_partition.csv', index=False)\n",
    "pd.DataFrame(part_t_rows, columns=['user','community_toxic']).to_csv('artifacts/user_partition_toxic.csv', index=False)\n",
    "\n",
    "comm_report = {\n",
    "    'n_users_original': int(len(users)),\n",
    "    'n_users_filtered': int(U_sub.number_of_nodes()),\n",
    "    'sampled': sampled_flag,\n",
    "    'min_degree_filter': min_deg,\n",
    "    'user_edges_all_filtered': int(U_sub.number_of_edges()),\n",
    "    'user_edges_toxic_filtered': int(U_toxic.number_of_edges()),\n",
    "    'n_communities_all': int(len(comms)),\n",
    "    'n_communities_toxic': int(len(comms_t)),\n",
    "    'modularity_all': mod,\n",
    "    'modularity_toxic': mod_t,\n",
    "    'ei_index_all': ei,\n",
    "    'ei_index_toxic': ei_t\n",
    "}\n",
    "with open('artifacts/communities_report.json','w') as f:\n",
    "    json.dump(comm_report, f, indent=2)\n",
    "print('Wrote artifacts/user_partition.csv, user_partition_toxic.csv, communities_report.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
