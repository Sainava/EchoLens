{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ff7f4e",
   "metadata": {},
   "source": [
    "# Toxic Comment Heterogeneous GNN Training Notebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4b46f",
   "metadata": {},
   "source": [
    "## 1. Load Context Configuration (Parse context.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97991e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG =>\n",
      "{\n",
      "  \"data_csv\": \"Notebooks/youtube_comments_with_toxicity_20250914_061551.csv\",\n",
      "  \"user_col\": \"AuthorChannelID\",\n",
      "  \"comment_col\": \"CommentText\",\n",
      "  \"comment_id_col\": \"CommentID\",\n",
      "  \"parent_col\": \"ParentCommentID\",\n",
      "  \"label_col\": \"ToxicLabel\",\n",
      "  \"score_col\": \"ToxicScore\",\n",
      "  \"binary_label_col\": \"ToxicBinary\",\n",
      "  \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
      "  \"gnn_hidden\": 128,\n",
      "  \"gnn_out_classes\": 2,\n",
      "  \"gnn_dropout\": 0.3,\n",
      "  \"gnn_num_layers\": 2,\n",
      "  \"use_class_weights\": true,\n",
      "  \"focal_loss\": false,\n",
      "  \"focal_gamma\": 2.0,\n",
      "  \"add_knn_similarity\": true,\n",
      "  \"knn_k\": 5,\n",
      "  \"knn_max_nodes\": 50000,\n",
      "  \"train_val_test_split\": [\n",
      "    0.8,\n",
      "    0.1,\n",
      "    0.1\n",
      "  ],\n",
      "  \"primary_metric\": \"f1\",\n",
      "  \"epochs\": 30,\n",
      "  \"lr\": 0.001,\n",
      "  \"weight_decay\": 1e-05,\n",
      "  \"early_stopping_patience\": 5,\n",
      "  \"batch_size_node_loader\": 1024,\n",
      "  \"neighbors\": 10,\n",
      "  \"community_min_degree\": 2,\n",
      "  \"community_max_nodes\": 100000,\n",
      "  \"motif_max_nodes\": 200000,\n",
      "  \"edge_classifier_epochs\": 20,\n",
      "  \"edge_classifier_balance\": true,\n",
      "  \"seed\": 42\n",
      "}\n",
      "Loaded dataframe shape: (1032225, 14)\n",
      "Loaded dataframe shape: (1032225, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentID</th>\n",
       "      <th>VideoID</th>\n",
       "      <th>VideoTitle</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>AuthorChannelID</th>\n",
       "      <th>CommentText</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Replies</th>\n",
       "      <th>PublishedAt</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>ToxicLabel</th>\n",
       "      <th>ToxicScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UgyRjrEdJIPrf68uND14AaABAg</td>\n",
       "      <td>mcY4M9gjtsI</td>\n",
       "      <td>They killed my friend.#tales #movie #shorts</td>\n",
       "      <td>@OneWhoWandered</td>\n",
       "      <td>UC_-UEXaBL1dqqUPGkDll49A</td>\n",
       "      <td>Anyone know what movie this is?</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-15 00:54:55</td>\n",
       "      <td>NZ</td>\n",
       "      <td>1</td>\n",
       "      <td>non-toxic</td>\n",
       "      <td>0.998745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UgxXxEIySAwnMNw8D7N4AaABAg</td>\n",
       "      <td>2vuXcw9SZbA</td>\n",
       "      <td>Man Utd conceding first penalty at home in yea...</td>\n",
       "      <td>@chiefvon3068</td>\n",
       "      <td>UCZ1LcZESjYqzaQRhjdZJFwg</td>\n",
       "      <td>The fact they're holding each other back while...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-13 23:51:46</td>\n",
       "      <td>AU</td>\n",
       "      <td>17</td>\n",
       "      <td>non-toxic</td>\n",
       "      <td>0.996063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    CommentID      VideoID  \\\n",
       "0  UgyRjrEdJIPrf68uND14AaABAg  mcY4M9gjtsI   \n",
       "1  UgxXxEIySAwnMNw8D7N4AaABAg  2vuXcw9SZbA   \n",
       "\n",
       "                                          VideoTitle       AuthorName  \\\n",
       "0        They killed my friend.#tales #movie #shorts  @OneWhoWandered   \n",
       "1  Man Utd conceding first penalty at home in yea...    @chiefvon3068   \n",
       "\n",
       "            AuthorChannelID  \\\n",
       "0  UC_-UEXaBL1dqqUPGkDll49A   \n",
       "1  UCZ1LcZESjYqzaQRhjdZJFwg   \n",
       "\n",
       "                                         CommentText Sentiment  Likes  \\\n",
       "0                    Anyone know what movie this is?   Neutral      0   \n",
       "1  The fact they're holding each other back while...  Positive      0   \n",
       "\n",
       "   Replies          PublishedAt CountryCode  CategoryID ToxicLabel  ToxicScore  \n",
       "0        2  2025-01-15 00:54:55          NZ           1  non-toxic    0.998745  \n",
       "1        0  2025-01-13 23:51:46          AU          17  non-toxic    0.996063  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, json, os, textwrap\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "CONTEXT_FILE = Path('context.md')\n",
    "assert CONTEXT_FILE.exists(), 'context.md not found in workspace.'\n",
    "raw = CONTEXT_FILE.read_text(encoding='utf-8')\n",
    "\n",
    "# heuristic extraction of CSV path\n",
    "m = re.search(r'youtube_comments_with_toxicity_(\\d+_\\d+)\\.csv', raw)\n",
    "if m:\n",
    "    csv_candidates = list(Path('Notebooks').glob(f'youtube_comments_with_toxicity_{m.group(1)}.csv'))\n",
    "else:\n",
    "    csv_candidates = list(Path('Notebooks').glob('youtube_comments_with_toxicity_*.csv'))\n",
    "DATA_CSV = csv_candidates[0] if csv_candidates else None\n",
    "\n",
    "config = {\n",
    "    'data_csv': str(DATA_CSV) if DATA_CSV else None,\n",
    "    'user_col': 'AuthorChannelID',\n",
    "    'comment_col': 'CommentText',\n",
    "    'comment_id_col': 'CommentID',\n",
    "    'parent_col': 'ParentCommentID',\n",
    "    'label_col': 'ToxicLabel',\n",
    "    'score_col': 'ToxicScore',\n",
    "    'binary_label_col': 'ToxicBinary',\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',\n",
    "    'gnn_hidden': 128,\n",
    "    'gnn_out_classes': 2,\n",
    "    'gnn_dropout': 0.3,\n",
    "    'gnn_num_layers': 2,\n",
    "    'use_class_weights': True,\n",
    "    'focal_loss': False,          # set True to experiment later\n",
    "    'focal_gamma': 2.0,\n",
    "    'add_knn_similarity': True,\n",
    "    'knn_k': 5,\n",
    "    'knn_max_nodes': 50000,       # cap for kNN (sampled subset when many comments)\n",
    "    'train_val_test_split': [0.8,0.1,0.1],\n",
    "    'primary_metric': 'f1',\n",
    "    'epochs': 30,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'early_stopping_patience': 5,\n",
    "    'batch_size_node_loader': 1024,\n",
    "    'neighbors': 10,\n",
    "    'community_min_degree': 2,    # filter users below this degree before community detection\n",
    "    'community_max_nodes': 100000, # skip / downsample if exceeds\n",
    "    'motif_max_nodes': 200000,    # skip detailed motif / triad census if bigger\n",
    "    'edge_classifier_epochs': 20,\n",
    "    'edge_classifier_balance': True,\n",
    "    'seed': 42\n",
    "}\n",
    "print('CONFIG =>')\n",
    "print(json.dumps(config, indent=2))\n",
    "\n",
    "if not DATA_CSV:\n",
    "    raise FileNotFoundError('Could not locate toxicity CSV. Please place it under Notebooks/.')\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "print('Loaded dataframe shape:', df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c05eb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied config overrides:\n",
      "{\n",
      "  \"focal_loss\": true,\n",
      "  \"focal_gamma\": 2.0,\n",
      "  \"lr\": 0.0005,\n",
      "  \"early_stopping_patience\": 10,\n",
      "  \"epochs\": 50\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 1b. Config overrides for stability and toxic recall\n",
    "# Safely override a few knobs without changing the original defaults above.\n",
    "overrides = {\n",
    "    'focal_loss': True,           # enable focal loss to up-weight hard (toxic) examples\n",
    "    'focal_gamma': 2.0,           # default gamma\n",
    "    'lr': 5e-4,                   # lower LR for stability\n",
    "    'early_stopping_patience': 10,# allow more patience before stopping\n",
    "    'epochs': 50                  # train a bit longer with early stopping\n",
    "}\n",
    "config.update(overrides)\n",
    "print('Applied config overrides:')\n",
    "print(json.dumps(overrides, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f0a3c",
   "metadata": {},
   "source": [
    "## 2. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "124a9a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version OK\n",
      "Torch: 2.8.0\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch, random\n",
    "from pathlib import Path\n",
    "\n",
    "REQUIRED = ['pandas','numpy','torch','sklearn','tqdm']\n",
    "print('Python version OK')\n",
    "print('Torch:', torch.__version__)\n",
    "\n",
    "def set_seed(seed:int):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(config.get('seed', 42))\n",
    "\n",
    "device = torch.device('cpu')  # Force CPU to avoid MPS OOM\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1cc3b",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "66cabde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 1032225\n",
      "Class balance: ToxicBinary\n",
      "0    0.953549\n",
      "1    0.046451\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentID</th>\n",
       "      <th>VideoID</th>\n",
       "      <th>VideoTitle</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>AuthorChannelID</th>\n",
       "      <th>CommentText</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Replies</th>\n",
       "      <th>PublishedAt</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>CategoryID</th>\n",
       "      <th>ToxicLabel</th>\n",
       "      <th>ToxicScore</th>\n",
       "      <th>ToxicBinary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UgyRjrEdJIPrf68uND14AaABAg</td>\n",
       "      <td>mcY4M9gjtsI</td>\n",
       "      <td>They killed my friend.#tales #movie #shorts</td>\n",
       "      <td>@OneWhoWandered</td>\n",
       "      <td>UC_-UEXaBL1dqqUPGkDll49A</td>\n",
       "      <td>Anyone know what movie this is?</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-01-15 00:54:55</td>\n",
       "      <td>NZ</td>\n",
       "      <td>1</td>\n",
       "      <td>non-toxic</td>\n",
       "      <td>0.998745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UgxXxEIySAwnMNw8D7N4AaABAg</td>\n",
       "      <td>2vuXcw9SZbA</td>\n",
       "      <td>Man Utd conceding first penalty at home in yea...</td>\n",
       "      <td>@chiefvon3068</td>\n",
       "      <td>UCZ1LcZESjYqzaQRhjdZJFwg</td>\n",
       "      <td>The fact they're holding each other back while...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-13 23:51:46</td>\n",
       "      <td>AU</td>\n",
       "      <td>17</td>\n",
       "      <td>non-toxic</td>\n",
       "      <td>0.996063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UgxB0jh2Ur41mcXr5IB4AaABAg</td>\n",
       "      <td>papg2tsoFzg</td>\n",
       "      <td>Welcome to Javascript Course</td>\n",
       "      <td>@Abdulla-ip8qr</td>\n",
       "      <td>UCWBK35w5Swy1iF5xIbEyw3A</td>\n",
       "      <td>waiting next video will be?</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-07-06 13:18:16</td>\n",
       "      <td>IN</td>\n",
       "      <td>27</td>\n",
       "      <td>non-toxic</td>\n",
       "      <td>0.997976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    CommentID      VideoID  \\\n",
       "0  UgyRjrEdJIPrf68uND14AaABAg  mcY4M9gjtsI   \n",
       "1  UgxXxEIySAwnMNw8D7N4AaABAg  2vuXcw9SZbA   \n",
       "2  UgxB0jh2Ur41mcXr5IB4AaABAg  papg2tsoFzg   \n",
       "\n",
       "                                          VideoTitle       AuthorName  \\\n",
       "0        They killed my friend.#tales #movie #shorts  @OneWhoWandered   \n",
       "1  Man Utd conceding first penalty at home in yea...    @chiefvon3068   \n",
       "2                       Welcome to Javascript Course   @Abdulla-ip8qr   \n",
       "\n",
       "            AuthorChannelID  \\\n",
       "0  UC_-UEXaBL1dqqUPGkDll49A   \n",
       "1  UCZ1LcZESjYqzaQRhjdZJFwg   \n",
       "2  UCWBK35w5Swy1iF5xIbEyw3A   \n",
       "\n",
       "                                         CommentText Sentiment  Likes  \\\n",
       "0                    Anyone know what movie this is?   Neutral      0   \n",
       "1  The fact they're holding each other back while...  Positive      0   \n",
       "2                        waiting next video will be?   Neutral      1   \n",
       "\n",
       "   Replies          PublishedAt CountryCode  CategoryID ToxicLabel  \\\n",
       "0        2  2025-01-15 00:54:55          NZ           1  non-toxic   \n",
       "1        0  2025-01-13 23:51:46          AU          17  non-toxic   \n",
       "2        0  2020-07-06 13:18:16          IN          27  non-toxic   \n",
       "\n",
       "   ToxicScore  ToxicBinary  \n",
       "0    0.998745            0  \n",
       "1    0.996063            0  \n",
       "2    0.997976            0  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure required columns & create binary label\n",
    "USER_COL=config['user_col']; COMMENT_COL=config['comment_col']; CID_COL=config['comment_id_col']\n",
    "if 'ToxicBinary' not in df.columns:\n",
    "    if df['ToxicLabel'].dtype==object:\n",
    "        df['ToxicBinary']=df['ToxicLabel'].str.lower().str.startswith('toxic').astype(int)\n",
    "    else:\n",
    "        df['ToxicBinary']=(df['ToxicScore']>0.7).astype(int)\n",
    "\n",
    "# Drop rows missing essentials\n",
    "df = df.dropna(subset=[USER_COL, COMMENT_COL])\n",
    "\n",
    "print('Rows after cleaning:', len(df))\n",
    "print('Class balance:', df['ToxicBinary'].value_counts(normalize=True))\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ff63c",
   "metadata": {},
   "source": [
    "## 4. Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "923d5872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falling back to bag-of-words (hash) embeddings due to error: No module named 'sentence_transformers'\n",
      "Embeddings shape: (1032225, 4)\n",
      "Embeddings shape: (1032225, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model_name = config['embedding_model']\n",
    "    sbert = SentenceTransformer(model_name)\n",
    "    texts = df[COMMENT_COL].astype(str).tolist()\n",
    "    batch=256; embs=[]\n",
    "    for i in range(0,len(texts),batch):\n",
    "        embs.append(sbert.encode(texts[i:i+batch], show_progress_bar=False))\n",
    "    import numpy as np\n",
    "    embeddings = np.vstack(embs)\n",
    "except Exception as e:\n",
    "    print('Falling back to bag-of-words (hash) embeddings due to error:', e)\n",
    "    import numpy as np, hashlib\n",
    "    def hvec(t):\n",
    "        h = hashlib.md5(t.encode()).hexdigest()\n",
    "        return np.array([int(h[i:i+4],16)%10000 for i in range(0,16,4)],dtype=float)\n",
    "    embeddings = np.vstack([hvec(t) for t in df[COMMENT_COL].astype(str)])\n",
    "\n",
    "print('Embeddings shape:', embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "89f66e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(6333) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6334) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6334) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6335) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6336) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6335) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6336) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6337) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6338) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6337) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6338) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6339) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6339) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6340) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(6340) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (text-only) best val threshold 0.05 | F1=0.089\n",
      "Baseline test report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000     98427\n",
      "           1      0.046     1.000     0.089      4795\n",
      "\n",
      "    accuracy                          0.046    103222\n",
      "   macro avg      0.023     0.500     0.044    103222\n",
      "weighted avg      0.002     0.046     0.004    103222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sainavamodak/Desktop/FAFO/social/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/sainavamodak/Desktop/FAFO/social/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/sainavamodak/Desktop/FAFO/social/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# 4b. Text-only baseline on embeddings (logistic regression)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "X = embeddings\n",
    "y = df['ToxicBinary'].values\n",
    "\n",
    "# Prefer graph masks if already created; otherwise, create stratified splits here\n",
    "try:\n",
    "    train_ids = hetero['comment'].train_mask.cpu().numpy()\n",
    "    val_ids = hetero['comment'].val_mask.cpu().numpy()\n",
    "    test_ids = hetero['comment'].test_mask.cpu().numpy()\n",
    "except Exception:\n",
    "    idx = np.arange(len(y))\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, temp_idx = next(sss.split(idx, y))\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    val_rel, test_rel = next(sss2.split(temp_idx, y[temp_idx]))\n",
    "    val_idx = temp_idx[val_rel]; test_idx = temp_idx[test_rel]\n",
    "    train_ids = np.zeros(len(y), dtype=bool); train_ids[train_idx]=True\n",
    "    val_ids = np.zeros(len(y), dtype=bool); val_ids[val_idx]=True\n",
    "    test_ids = np.zeros(len(y), dtype=bool); test_ids[test_idx]=True\n",
    "\n",
    "logreg = LogisticRegression(max_iter=200, class_weight='balanced', n_jobs=-1)\n",
    "logreg.fit(X[train_ids], y[train_ids])\n",
    "probs = logreg.predict_proba(X[val_ids])[:,1]\n",
    "# Threshold tuning on validation\n",
    "best_t = 0.5; best_f1=-1\n",
    "for t in [i/100 for i in range(5,96)]:\n",
    "    pred = (probs>=t).astype(int)\n",
    "    f1 = f1_score(y[val_ids], pred, average='binary')\n",
    "    if f1>best_f1:\n",
    "        best_f1, best_t = f1, t\n",
    "print(f'Baseline (text-only) best val threshold {best_t:.2f} | F1={best_f1:.3f}')\n",
    "# Evaluate on test\n",
    "probs_test = logreg.predict_proba(X[test_ids])[:,1]\n",
    "preds_test = (probs_test>=best_t).astype(int)\n",
    "print('Baseline test report:\\n', classification_report(y[test_ids], preds_test, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a2050",
   "metadata": {},
   "source": [
    "## 5. Build Graph (HeteroData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cc6d7a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added similarity edges: 250257\n",
      "HeteroData(\n",
      "  comment={\n",
      "    x=[1032225, 4],\n",
      "    y=[1032225],\n",
      "  },\n",
      "  user={ x=[759619, 2] },\n",
      "  (user, authored, comment)={ edge_index=[2, 1032225] },\n",
      "  (comment, replies_to, comment)={ edge_index=[2, 1027662] },\n",
      "  (comment, similar, comment)={ edge_index=[2, 250257] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch, numpy as np\n",
    "\n",
    "# Reply edges heuristic if parent col missing (normalize IDs to str)\n",
    "if config['parent_col'] in df.columns and df[config['parent_col']].notna().any():\n",
    "    reply_pairs = (\n",
    "        df[df[config['parent_col']].notna()][[config['parent_col'], config['comment_id_col']]]\n",
    "        .astype(str)\n",
    "        .values\n",
    "        .tolist()\n",
    "    )\n",
    "else:\n",
    "    reply_pairs = []\n",
    "    if 'VideoID' in df.columns and 'PublishedAt' in df.columns:\n",
    "        df_sorted = df.sort_values(['VideoID','PublishedAt'])\n",
    "        for vid, group in df_sorted.groupby('VideoID'):\n",
    "            ids = group[config['comment_id_col']].astype(str).tolist()\n",
    "            for i in range(1, len(ids)):\n",
    "                reply_pairs.append((ids[i-1], ids[i]))\n",
    "\n",
    "# Canonicalize IDs to strings for consistent indexing\n",
    "comment_ids = df[CID_COL].astype(str).tolist()\n",
    "comment_idx = {cid: i for i, cid in enumerate(comment_ids)}\n",
    "users = df[USER_COL].astype(str).unique().tolist()\n",
    "user_idx = {u: i for i, u in enumerate(users)}\n",
    "\n",
    "# user->comment authored edges (normalize to str)\n",
    "authored_edges = [(str(row[USER_COL]), str(row[CID_COL])) for _, row in df.iterrows()]\n",
    "\n",
    "# Build features for users\n",
    "user_deg = {u: 0 for u in users}; user_tox = {u: [] for u in users}\n",
    "for _, r in df.iterrows():\n",
    "    u = str(r[USER_COL]); user_deg[u] += 1; user_tox[u].append(r['ToxicBinary'])\n",
    "user_feat = np.vstack([\n",
    "    [user_deg[u] for u in users],\n",
    "    [np.mean(user_tox[u]) if user_tox[u] else 0 for u in users]\n",
    "]).T\n",
    "\n",
    "hetero = HeteroData()\n",
    "hetero['comment'].x = torch.tensor(embeddings, dtype=torch.float)\n",
    "hetero['comment'].y = torch.tensor(df['ToxicBinary'].values, dtype=torch.long)\n",
    "hetero['user'].x = torch.tensor(user_feat, dtype=torch.float)\n",
    "\n",
    "# Build edge indices (guard for any stray IDs)\n",
    "src = [user_idx[str(u)] for u, c in authored_edges if str(u) in user_idx and str(c) in comment_idx]\n",
    "dst = [comment_idx[str(c)] for u, c in authored_edges if str(u) in user_idx and str(c) in comment_idx]\n",
    "hetero['user','authored','comment'].edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "r_src = [comment_idx[str(p)] for p, c in reply_pairs if str(p) in comment_idx and str(c) in comment_idx]\n",
    "r_dst = [comment_idx[str(c)] for p, c in reply_pairs if str(p) in comment_idx and str(c) in comment_idx]\n",
    "hetero['comment','replies_to','comment'].edge_index = torch.tensor([r_src, r_dst], dtype=torch.long)\n",
    "\n",
    "# Optional: add kNN similarity edges among comments to densify the graph\n",
    "if config.get('add_knn_similarity', False):\n",
    "    max_nodes = config.get('knn_max_nodes', 50000)\n",
    "    emb_tensor = hetero['comment'].x\n",
    "    total_nodes = emb_tensor.size(0)\n",
    "    if total_nodes > 10:  # only if meaningful\n",
    "        if total_nodes > max_nodes:\n",
    "            # sample a subset for similarity graph; map back indices\n",
    "            sample_idx = torch.randperm(total_nodes)[:max_nodes]\n",
    "            emb_sample = emb_tensor[sample_idx]\n",
    "            base_indices = sample_idx\n",
    "        else:\n",
    "            emb_sample = emb_tensor\n",
    "            base_indices = torch.arange(total_nodes)\n",
    "        # Normalize and compute approximate cosine similarity via inner product\n",
    "        with torch.no_grad():\n",
    "            normed = torch.nn.functional.normalize(emb_sample, p=2, dim=1)\n",
    "            # chunked to control memory\n",
    "            k = config.get('knn_k', 5)\n",
    "            edges_sim_src = []\n",
    "            edges_sim_dst = []\n",
    "            chunk = 2048\n",
    "            for start in range(0, normed.size(0), chunk):\n",
    "                blk = normed[start:start+chunk]\n",
    "                sim = blk @ normed.T  # [chunk, N]\n",
    "                topk = torch.topk(sim, k=k+1, dim=1).indices  # include self then filter\n",
    "                base_rows = base_indices[start:start+chunk]\n",
    "                for row_i, neighs in zip(base_rows.tolist(), topk):\n",
    "                    for n in neighs.tolist():\n",
    "                        if base_indices[n] != row_i:  # skip self\n",
    "                            edges_sim_src.append(row_i)\n",
    "                            edges_sim_dst.append(base_indices[n].item())\n",
    "            if edges_sim_src:\n",
    "                hetero['comment','similar','comment'].edge_index = torch.tensor([edges_sim_src, edges_sim_dst], dtype=torch.long)\n",
    "                print(f\"Added similarity edges: {len(edges_sim_src)}\")\n",
    "\n",
    "print(hetero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2348144",
   "metadata": {},
   "source": [
    "## 6. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "339257af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 825780 103222 103223\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_comments = hetero['comment'].num_nodes\n",
    "perm = torch.randperm(num_comments)\n",
    "n_train=int(config['train_val_test_split'][0]*num_comments)\n",
    "n_val=int(config['train_val_test_split'][1]*num_comments)\n",
    "train_idx=perm[:n_train]; val_idx=perm[n_train:n_train+n_val]; test_idx=perm[n_train+n_val:]\n",
    "train_mask=torch.zeros(num_comments,dtype=torch.bool); train_mask[train_idx]=True\n",
    "val_mask=torch.zeros(num_comments,dtype=torch.bool); val_mask[val_idx]=True\n",
    "test_mask=torch.zeros(num_comments,dtype=torch.bool); test_mask[test_idx]=True\n",
    "hetero['comment'].train_mask=train_mask\n",
    "hetero['comment'].val_mask=val_mask\n",
    "hetero['comment'].test_mask=test_mask\n",
    "print('Split sizes:', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "50a4ad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified split sizes: 825780 103223 103222\n",
      "Class dist train/val/test: 0.046450628496694034 0.046452825436191544 0.046453275464532755\n",
      "Class dist train/val/test: 0.046450628496694034 0.046452825436191544 0.046453275464532755\n"
     ]
    }
   ],
   "source": [
    "# 6b. Stratified split by ToxicBinary for stability\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "labels = hetero['comment'].y.detach().cpu().numpy()\n",
    "idx = np.arange(len(labels))\n",
    "# First split: train vs (val+test)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=config['train_val_test_split'][1]+config['train_val_test_split'][2], random_state=config.get('seed',42))\n",
    "train_idx, temp_idx = next(sss.split(idx, labels))\n",
    "\n",
    "# Second split: split temp into val and test\n",
    "val_prop = config['train_val_test_split'][1]\n",
    "val_test_total = config['train_val_test_split'][1] + config['train_val_test_split'][2]\n",
    "val_size = val_prop / val_test_total if val_test_total > 0 else 0.5\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=config.get('seed',42))\n",
    "# Here, \"test\" from sss2 will represent the VAL set, and \"train\" will be TEST\n",
    "test_rel, val_rel = next(sss2.split(temp_idx, labels[temp_idx]))\n",
    "val_idx = temp_idx[val_rel]\n",
    "test_idx = temp_idx[test_rel]\n",
    "\n",
    "train_mask=torch.zeros(len(labels),dtype=torch.bool); train_mask[train_idx]=True\n",
    "val_mask=torch.zeros(len(labels),dtype=torch.bool); val_mask[val_idx]=True\n",
    "test_mask=torch.zeros(len(labels),dtype=torch.bool); test_mask[test_idx]=True\n",
    "hetero['comment'].train_mask=train_mask\n",
    "hetero['comment'].val_mask=val_mask\n",
    "hetero['comment'].test_mask=test_mask\n",
    "\n",
    "print('Stratified split sizes:', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())\n",
    "print('Class dist train/val/test:', labels[train_mask.cpu().numpy()].mean(), labels[val_mask.cpu().numpy()].mean(), labels[test_mask.cpu().numpy()].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c1100",
   "metadata": {},
   "source": [
    "## 7. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "10d7c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroGNN(\n",
      "  (in_lins): ModuleDict(\n",
      "    (user): Linear(-1, 128, bias=True)\n",
      "    (comment): Linear(-1, 128, bias=True)\n",
      "  )\n",
      "  (convs): ModuleList(\n",
      "    (0-1): 2 x HeteroConv(num_relations=3)\n",
      "  )\n",
      "  (lin): Linear(128, 2, bias=True)\n",
      ")\n",
      "Model and data both on: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sainavamodak/Desktop/FAFO/social/lib/python3.12/site-packages/torch_geometric/nn/conv/hetero_conv.py:76: UserWarning: There exist node types ({'user'}) whose representations do not get updated during message passing as they do not occur as destination type in any edge type. This may lead to unexpected behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Build metadata-driven multi-layer hetero GNN with dropout\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden, out_classes, num_layers=2, dropout=0.3, edge_types=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.hidden = hidden\n",
    "        self.out_classes = out_classes\n",
    "        rels = set(edge_types or [])\n",
    "        if not rels:\n",
    "            raise ValueError('HeteroGNN requires at least one edge type; got none.')\n",
    "        # Type-wise input projection to hidden dim (lazy Linear infers input dim on first use)\n",
    "        self.in_lins = nn.ModuleDict({\n",
    "            'user': Linear(-1, hidden),\n",
    "            'comment': Linear(-1, hidden),\n",
    "        })\n",
    "        # Helper to build relation dict depending on available relations\n",
    "        def build_rel_dict():\n",
    "            rel_dict = {}\n",
    "            if ('user','authored','comment') in rels:\n",
    "                rel_dict[('user','authored','comment')] = SAGEConv((hidden, hidden), hidden)\n",
    "            if ('comment','replies_to','comment') in rels:\n",
    "                rel_dict[('comment','replies_to','comment')] = SAGEConv((hidden, hidden), hidden)\n",
    "            if ('comment','similar','comment') in rels:\n",
    "                rel_dict[('comment','similar','comment')] = SAGEConv((hidden, hidden), hidden)\n",
    "            return rel_dict\n",
    "        # All layers operate on hidden-sized features\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(HeteroConv(build_rel_dict(), aggr='mean'))\n",
    "        self.lin = Linear(hidden, out_classes)\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Project raw node features to hidden size\n",
    "        x_dict = {k: self.in_lins[k](v) if k in self.in_lins else v for k, v in x_dict.items()}\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            out_dict = conv(x_dict, edge_index_dict)\n",
    "            # Merge: keep previous embeddings for node types not updated in this layer\n",
    "            merged = {}\n",
    "            for k in x_dict.keys():\n",
    "                v = out_dict.get(k, x_dict[k])\n",
    "                merged[k] = F.dropout(v.relu(), p=self.dropout, training=self.training)\n",
    "            x_dict = merged\n",
    "        return self.lin(x_dict['comment'])\n",
    "\n",
    "model = HeteroGNN(\n",
    "    config['gnn_hidden'],\n",
    "    config['gnn_out_classes'],\n",
    "    config['gnn_num_layers'],\n",
    "    config['gnn_dropout'],\n",
    "    edge_types=hetero.edge_types\n",
    ").to(device)\n",
    "# Ensure hetero data is also on the same device as model\n",
    "hetero = hetero.to(device)\n",
    "print(model)\n",
    "print(f'Model and data both on: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4cd8f5",
   "metadata": {},
   "source": [
    "## 8. Training Hyperparameters & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ade09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "\n",
    "# Optional focal loss implementation\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, target):\n",
    "        ce = torch.nn.functional.cross_entropy(logits, target, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1-pt)**self.gamma) * ce\n",
    "        if self.reduction=='mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction=='sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# Class weights for imbalance\n",
    "if config.get('use_class_weights', False):\n",
    "    y_all = hetero['comment'].y.detach().cpu().numpy()\n",
    "    counts = np.bincount(y_all)\n",
    "    total = counts.sum()\n",
    "    weights = total / (len(counts) * counts)\n",
    "    class_w_tensor = torch.tensor(weights, dtype=torch.float, device=device)\n",
    "else:\n",
    "    class_w_tensor = None\n",
    "\n",
    "if config.get('focal_loss', False):\n",
    "    criterion = FocalLoss(gamma=config.get('focal_gamma', 2.0), weight=class_w_tensor)\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_w_tensor)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "# Gradient clipping norm (define before training loop)\n",
    "max_norm = 1.0\n",
    "\n",
    "best_metric=-1.0\n",
    "patience=0\n",
    "best_path = Path('model_best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2a130",
   "metadata": {},
   "source": [
    "## 9. Training Loop with Validation & Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a0afa",
   "metadata": {},
   "source": [
    "## 8.5. Device Sync and Diagnostic Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5f3f1e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Device Sync Check ===\n",
      "Target device: cpu\n",
      "hetero.x_dict devices: ['comment:cpu', 'user:cpu']\n",
      "Model device: cpu\n",
      "Class weights device: cpu\n",
      "Testing forward pass...\n",
      "✅ Forward pass successful! Output shape: torch.Size([1032225, 2])\n",
      "=== Ready for training ===\n",
      "✅ Forward pass successful! Output shape: torch.Size([1032225, 2])\n",
      "=== Ready for training ===\n"
     ]
    }
   ],
   "source": [
    "# Force everything to CPU and verify device consistency\n",
    "print(\"=== Device Sync Check ===\")\n",
    "print(f\"Target device: {device}\")\n",
    "\n",
    "# Move hetero graph to CPU (may already be there, but ensure consistency)\n",
    "hetero = hetero.cpu()\n",
    "print(f\"hetero.x_dict devices: {[f'{k}:{v.device}' for k,v in hetero.x_dict.items()]}\")\n",
    "\n",
    "# Recreate model and optimizer on CPU (fresh start)\n",
    "model = HeteroGNN(\n",
    "    config['gnn_hidden'],\n",
    "    config['gnn_out_classes'],\n",
    "    config['gnn_num_layers'],\n",
    "    config['gnn_dropout'],\n",
    "    edge_types=hetero.edge_types\n",
    ").to(device)\n",
    "\n",
    "# Recreate optimizer and criterion for CPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "if config.get('focal_loss', False):\n",
    "    criterion = FocalLoss(gamma=config.get('focal_gamma', 2.0), weight=class_w_tensor.to(device) if class_w_tensor is not None else None)\n",
    "else:\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_w_tensor.to(device) if class_w_tensor is not None else None)\n",
    "\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Class weights device: {class_w_tensor.device if class_w_tensor is not None else 'None'}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"Testing forward pass...\")\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        test_out = model(hetero.x_dict, hetero.edge_index_dict)\n",
    "        print(f\"✅ Forward pass successful! Output shape: {test_out.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Forward pass failed: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"=== Ready for training ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4cc2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss 292.0178 | val_f1 0.4854 | val_acc 0.8169\n",
      "  ✅ Saved new best model (F1=0.4854)\n",
      "Epoch 02 | loss 568.3104 | val_f1 0.4861 | val_acc 0.8203\n",
      "  ✅ Saved new best model (F1=0.4861)\n",
      "Epoch 02 | loss 568.3104 | val_f1 0.4861 | val_acc 0.8203\n",
      "  ✅ Saved new best model (F1=0.4861)\n",
      "Epoch 03 | loss 354.9366 | val_f1 0.4881 | val_acc 0.9534\n",
      "  ✅ Saved new best model (F1=0.4881)\n",
      "Epoch 03 | loss 354.9366 | val_f1 0.4881 | val_acc 0.9534\n",
      "  ✅ Saved new best model (F1=0.4881)\n",
      "Epoch 04 | loss 607.1896 | val_f1 0.4917 | val_acc 0.9493\n",
      "  ✅ Saved new best model (F1=0.4917)\n",
      "Epoch 04 | loss 607.1896 | val_f1 0.4917 | val_acc 0.9493\n",
      "  ✅ Saved new best model (F1=0.4917)\n",
      "Epoch 05 | loss 273.7015 | val_f1 0.4661 | val_acc 0.7513\n",
      "Epoch 05 | loss 273.7015 | val_f1 0.4661 | val_acc 0.7513\n",
      "Epoch 06 | loss 301.7830 | val_f1 0.2055 | val_acc 0.2230\n",
      "Epoch 06 | loss 301.7830 | val_f1 0.2055 | val_acc 0.2230\n",
      "Epoch 07 | loss 215.7070 | val_f1 0.1703 | val_acc 0.1786\n",
      "Epoch 07 | loss 215.7070 | val_f1 0.1703 | val_acc 0.1786\n",
      "Epoch 08 | loss 223.5988 | val_f1 0.0447 | val_acc 0.0467\n",
      "Epoch 08 | loss 223.5988 | val_f1 0.0447 | val_acc 0.0467\n",
      "Epoch 09 | loss 186.8982 | val_f1 0.0450 | val_acc 0.0470\n",
      "Epoch 09 | loss 186.8982 | val_f1 0.0450 | val_acc 0.0470\n",
      "Epoch 10 | loss 232.2803 | val_f1 0.0511 | val_acc 0.0525\n",
      "Epoch 10 | loss 232.2803 | val_f1 0.0511 | val_acc 0.0525\n",
      "Epoch 11 | loss 251.3195 | val_f1 0.1006 | val_acc 0.1008\n",
      "Epoch 11 | loss 251.3195 | val_f1 0.1006 | val_acc 0.1008\n",
      "Epoch 12 | loss 221.5345 | val_f1 0.2944 | val_acc 0.3552\n",
      "Epoch 12 | loss 221.5345 | val_f1 0.2944 | val_acc 0.3552\n",
      "Epoch 13 | loss 154.7392 | val_f1 0.4605 | val_acc 0.7340\n",
      "Epoch 13 | loss 154.7392 | val_f1 0.4605 | val_acc 0.7340\n",
      "Epoch 14 | loss 160.1339 | val_f1 0.4792 | val_acc 0.8061\n",
      "Early stopping triggered.\n",
      "Epoch 14 | loss 160.1339 | val_f1 0.4792 | val_acc 0.8061\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>292.017792</td>\n",
       "      <td>0.485422</td>\n",
       "      <td>0.816882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>568.310364</td>\n",
       "      <td>0.486095</td>\n",
       "      <td>0.820302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>354.936615</td>\n",
       "      <td>0.488083</td>\n",
       "      <td>0.953441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>607.189636</td>\n",
       "      <td>0.491725</td>\n",
       "      <td>0.949304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>273.701538</td>\n",
       "      <td>0.466055</td>\n",
       "      <td>0.751344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch        loss    val_f1   val_acc\n",
       "0      1  292.017792  0.485422  0.816882\n",
       "1      2  568.310364  0.486095  0.820302\n",
       "2      3  354.936615  0.488083  0.953441\n",
       "3      4  607.189636  0.491725  0.949304\n",
       "4      5  273.701538  0.466055  0.751344"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "history = []\n",
    "for epoch in range(1, config['epochs']+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(hetero.x_dict, hetero.edge_index_dict)\n",
    "    y = hetero['comment'].y\n",
    "    loss = criterion(out[hetero['comment'].train_mask], y[hetero['comment'].train_mask])\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_eval = model(hetero.x_dict, hetero.edge_index_dict)\n",
    "        val_logits = logits_eval[hetero['comment'].val_mask]\n",
    "        val_y = y[hetero['comment'].val_mask]\n",
    "        preds = val_logits.argmax(dim=1).cpu().numpy()\n",
    "        val_y_np = val_y.cpu().numpy()\n",
    "        f1 = f1_score(val_y_np, preds, average='macro')\n",
    "        acc = accuracy_score(val_y_np, preds)\n",
    "    history.append({'epoch':epoch,'loss':float(loss.item()),'val_f1':float(f1),'val_acc':float(acc)})\n",
    "    print(f\"Epoch {epoch:02d} | loss {loss.item():.4f} | val_f1 {f1:.4f} | val_acc {acc:.4f}\")\n",
    "\n",
    "    if f1>best_metric:\n",
    "        best_metric=f1; patience=0\n",
    "        torch.save({'model_state': model.state_dict(), 'config': config}, best_path)\n",
    "        print('   Saved new best model (F1={:.4f})'.format(f1))\n",
    "    else:\n",
    "        patience+=1\n",
    "        if patience>=config['early_stopping_patience']:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "import pandas as _pd\n",
    "hist_df = _pd.DataFrame(history)\n",
    "hist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fb6f96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9b. Gradient clipping for stability\n",
    "max_norm = 1.0\n",
    "\n",
    "# 9c. Threshold tuning helper\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def tune_threshold(logits, y_true, metric='f1', average='binary'):\n",
    "    # logits -> probabilities for class 1\n",
    "    probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "    y_np = y_true.cpu().numpy()\n",
    "    best_t, best_f1 = 0.5, -1\n",
    "    for t in [i/100 for i in range(5, 96)]:\n",
    "        pred = (probs >= t).astype(int)\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_np, pred, average=average, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t, best_p, best_r = f1, t, p, r\n",
    "    return best_t, best_p, best_r, best_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08e3b1",
   "metadata": {},
   "source": [
    "## 10. Test Evaluation (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6f82008d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test (argmax/0.5) MacroF1=0.4917 Acc=0.9497\n",
      "Best threshold: 0.19 | P=0.059 R=0.006 F1=0.011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.995     0.974     98427\n",
      "           1      0.059     0.006     0.011      4795\n",
      "\n",
      "    accuracy                          0.949    103222\n",
      "   macro avg      0.506     0.501     0.492    103222\n",
      "weighted avg      0.912     0.949     0.929    103222\n",
      "\n",
      "Confusion matrix:\n",
      " [[97977   450]\n",
      " [ 4767    28]]\n",
      "Best threshold: 0.19 | P=0.059 R=0.006 F1=0.011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.954     0.995     0.974     98427\n",
      "           1      0.059     0.006     0.011      4795\n",
      "\n",
      "    accuracy                          0.949    103222\n",
      "   macro avg      0.506     0.501     0.492    103222\n",
      "weighted avg      0.912     0.949     0.929    103222\n",
      "\n",
      "Confusion matrix:\n",
      " [[97977   450]\n",
      " [ 4767    28]]\n"
     ]
    }
   ],
   "source": [
    "# Load best and evaluate on test with threshold tuning\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "state = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(state['model_state'])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(hetero.x_dict, hetero.edge_index_dict)\n",
    "    test_logits = logits[hetero['comment'].test_mask]\n",
    "    test_y = hetero['comment'].y[hetero['comment'].test_mask]\n",
    "\n",
    "# Default 0.5 threshold metrics\n",
    "preds_05 = test_logits.argmax(dim=1).cpu().numpy()\n",
    "y_true = test_y.cpu().numpy()\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "f1_05 = f1_score(y_true, preds_05, average='macro')\n",
    "acc_05 = accuracy_score(y_true, preds_05)\n",
    "print(f'Test (argmax/0.5) MacroF1={f1_05:.4f} Acc={acc_05:.4f}')\n",
    "\n",
    "# Tune threshold for toxic class F1\n",
    "best_t, best_p, best_r, best_f1 = tune_threshold(test_logits, test_y, average='binary')\n",
    "print(f'Best threshold: {best_t:.2f} | P={best_p:.3f} R={best_r:.3f} F1={best_f1:.3f}')\n",
    "\n",
    "# Report per-class metrics at best threshold\n",
    "probs = torch.softmax(test_logits, dim=1)[:,1].cpu().numpy()\n",
    "preds_best = (probs >= best_t).astype(int)\n",
    "print(classification_report(y_true, preds_best, digits=3))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_true, preds_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "90dc11bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary head not found; skipping temperature calibration and using T=1.0\n"
     ]
    }
   ],
   "source": [
    "# 10d. Temperature scaling calibration on validation\n",
    "import torch\n",
    "try:\n",
    "    head\n",
    "except NameError:\n",
    "    # If head isn't trained yet, skip calibration and set T=1.0\n",
    "    T = torch.tensor(1.0, device=device)\n",
    "    print('Binary head not found; skipping temperature calibration and using T=1.0')\n",
    "else:\n",
    "    # Fit a scalar T that minimizes BCE on validation logits\n",
    "    if 'T' not in globals():\n",
    "        T = torch.nn.Parameter(torch.tensor(1.0, device=device))\n",
    "    else:\n",
    "        # ensure it's a Parameter for LBFGS\n",
    "        T = torch.nn.Parameter(T.detach().clone().to(device))\n",
    "    optim_T = torch.optim.LBFGS([T], lr=0.1, max_iter=50)\n",
    "\n",
    "    val_logits_detached = None\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits_detached = head(X_all[val_idx]).detach()\n",
    "    val_y_detached = y_bin[val_idx].detach()\n",
    "\n",
    "    def _closure():\n",
    "        optim_T.zero_grad()\n",
    "        probs = torch.sigmoid(val_logits_detached / T)\n",
    "        # small epsilon to avoid log(0) in BCE; use BCE with logits equivalent by clamping probs\n",
    "        eps = 1e-7\n",
    "        probs = torch.clamp(probs, eps, 1-eps)\n",
    "        loss = -(val_y_detached*torch.log(probs) + (1-val_y_detached)*torch.log(1-probs)).mean()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    try:\n",
    "        optim_T.step(_closure)\n",
    "        print('Fitted temperature T =', float(T.item()))\n",
    "    except Exception as e:\n",
    "        print('Temperature scaling failed, using T=1.0. Error:', e)\n",
    "        with torch.no_grad():\n",
    "            T.copy_(torch.tensor(1.0, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c310189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10c. Tune threshold helper for binary logits (sigmoid)\n",
    "def tune_threshold_binary(probs_np, y_np):\n",
    "    best_t, best_f1, best_p, best_r = 0.5, -1, 0, 0\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    # finer/wider sweep including very low thresholds for recall\n",
    "    grid = [i/1000 for i in range(1, 999)]\n",
    "    for t in grid:\n",
    "        pred = (probs_np >= t).astype(int)\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_np, pred, average='binary', zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_t, best_f1, best_p, best_r = t, f1, p, r\n",
    "    return best_t, best_p, best_r, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4b0fe668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight: 20.528234481811523\n",
      "[BCE] Epoch 01 | loss 4981.4717 | val F1 0.0889 @ thr 0.86\n",
      "[BCE] Epoch 01 | loss 4981.4717 | val F1 0.0889 @ thr 0.86\n",
      "[BCE] Epoch 02 | loss 859.2150 | val F1 0.0888 @ thr 0.89\n",
      "[BCE] Epoch 02 | loss 859.2150 | val F1 0.0888 @ thr 0.89\n",
      "[BCE] Epoch 03 | loss 853.3982 | val F1 0.0890 @ thr 0.96\n",
      "[BCE] Epoch 03 | loss 853.3982 | val F1 0.0890 @ thr 0.96\n",
      "[BCE] Epoch 04 | loss 847.2483 | val F1 0.0888 @ thr 0.94\n",
      "[BCE] Epoch 04 | loss 847.2483 | val F1 0.0888 @ thr 0.94\n",
      "[BCE] Epoch 05 | loss 845.2862 | val F1 0.0889 @ thr 0.94\n",
      "[BCE] Epoch 05 | loss 845.2862 | val F1 0.0889 @ thr 0.94\n",
      "[BCE] Epoch 06 | loss 842.0702 | val F1 0.0890 @ thr 0.94\n",
      "[BCE] Epoch 06 | loss 842.0702 | val F1 0.0890 @ thr 0.94\n",
      "[BCE] Epoch 07 | loss 837.7093 | val F1 0.0888 @ thr 0.71\n",
      "[BCE] Epoch 07 | loss 837.7093 | val F1 0.0888 @ thr 0.71\n",
      "[BCE] Epoch 08 | loss 836.0605 | val F1 0.0889 @ thr 0.91\n",
      "[BCE] Epoch 08 | loss 836.0605 | val F1 0.0889 @ thr 0.91\n",
      "[BCE] Epoch 09 | loss 833.9459 | val F1 0.0890 @ thr 0.90\n",
      "[BCE] Epoch 09 | loss 833.9459 | val F1 0.0890 @ thr 0.90\n",
      "[BCE] Epoch 10 | loss 831.8811 | val F1 0.0890 @ thr 0.92\n",
      "[BCE] Epoch 10 | loss 831.8811 | val F1 0.0890 @ thr 0.92\n",
      "[BCE] Epoch 11 | loss 829.7872 | val F1 0.0889 @ thr 0.92\n",
      "[BCE] Epoch 11 | loss 829.7872 | val F1 0.0889 @ thr 0.92\n",
      "[BCE] Epoch 12 | loss 830.8763 | val F1 0.0889 @ thr 0.95\n",
      "[BCE] Epoch 12 | loss 830.8763 | val F1 0.0889 @ thr 0.95\n",
      "[BCE] Epoch 13 | loss 839.7443 | val F1 0.0889 @ thr 0.87\n",
      "[BCE] Epoch 13 | loss 839.7443 | val F1 0.0889 @ thr 0.87\n",
      "[BCE] Epoch 14 | loss 823.7784 | val F1 0.0891 @ thr 0.93\n",
      "[BCE] Epoch 14 | loss 823.7784 | val F1 0.0891 @ thr 0.93\n",
      "[BCE] Epoch 15 | loss 823.1734 | val F1 0.0890 @ thr 0.94\n",
      "[BCE] Epoch 15 | loss 823.1734 | val F1 0.0890 @ thr 0.94\n",
      "[BCE] Epoch 16 | loss 820.5782 | val F1 0.0890 @ thr 0.93\n",
      "[BCE] Epoch 16 | loss 820.5782 | val F1 0.0890 @ thr 0.93\n",
      "[BCE] Epoch 17 | loss 821.0845 | val F1 0.0889 @ thr 0.94\n",
      "[BCE] Epoch 17 | loss 821.0845 | val F1 0.0889 @ thr 0.94\n",
      "[BCE] Epoch 18 | loss 819.9059 | val F1 0.0890 @ thr 0.94\n",
      "[BCE] Epoch 18 | loss 819.9059 | val F1 0.0890 @ thr 0.94\n",
      "[BCE] Epoch 19 | loss 818.8621 | val F1 0.0888 @ thr 0.06\n",
      "[BCE] Epoch 19 | loss 818.8621 | val F1 0.0888 @ thr 0.06\n",
      "[BCE] Epoch 20 | loss 841.5857 | val F1 0.0889 @ thr 0.95\n",
      "[BCE] Epoch 20 | loss 841.5857 | val F1 0.0889 @ thr 0.95\n",
      "[BCE] Epoch 21 | loss 816.8452 | val F1 0.0888 @ thr 0.69\n",
      "[BCE] Epoch 21 | loss 816.8452 | val F1 0.0888 @ thr 0.69\n",
      "[BCE] Epoch 22 | loss 815.1866 | val F1 0.0889 @ thr 0.92\n",
      "[BCE] Epoch 22 | loss 815.1866 | val F1 0.0889 @ thr 0.92\n",
      "[BCE] Epoch 23 | loss 814.5572 | val F1 0.0888 @ thr 0.51\n",
      "[BCE] Epoch 23 | loss 814.5572 | val F1 0.0888 @ thr 0.51\n",
      "[BCE] Epoch 24 | loss 814.0022 | val F1 0.0888 @ thr 0.92\n",
      "[BCE] Epoch 24 | loss 814.0022 | val F1 0.0888 @ thr 0.92\n",
      "[BCE] Epoch 25 | loss 816.1798 | val F1 0.0888 @ thr 0.93\n",
      "Best val F1 (BCE head): 0.08913025063765306\n",
      "[BCE] Epoch 25 | loss 816.1798 | val F1 0.0888 @ thr 0.93\n",
      "Best val F1 (BCE head): 0.08913025063765306\n"
     ]
    }
   ],
   "source": [
    "# 10b. Binary head training with BCEWithLogitsLoss + pos_weight and optional oversampling\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import WeightedRandomSampler, TensorDataset, DataLoader\n",
    "\n",
    "# 1) Get frozen comment embeddings from current GNN encoder (no classifier)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_dict_emb = {k: model.in_lins[k](v) if k in model.in_lins else v for k, v in hetero.x_dict.items()}\n",
    "    for conv in model.convs:\n",
    "        out_dict = conv(x_dict_emb, hetero.edge_index_dict)\n",
    "        x_dict_emb = {k: out_dict.get(k, x_dict_emb[k]).relu() for k in x_dict_emb.keys()}\n",
    "    hidden_comment = x_dict_emb['comment']  # [N, H]\n",
    "\n",
    "y = hetero['comment'].y.to(device)\n",
    "train_mask = hetero['comment'].train_mask.to(device)\n",
    "val_mask = hetero['comment'].val_mask.to(device)\n",
    "test_mask = hetero['comment'].test_mask.to(device)\n",
    "\n",
    "X_all = hidden_comment.to(device)\n",
    "y_bin = y.float()  # [0,1]\n",
    "\n",
    "# 2) Binary head\n",
    "class BinaryHead(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1)  # single logit\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)  # [N]\n",
    "\n",
    "head = BinaryHead(X_all.size(1), hidden=128, dropout=config.get('gnn_dropout',0.3)).to(device)\n",
    "opt_head = torch.optim.Adam(head.parameters(), lr=config.get('lr', 5e-4), weight_decay=config.get('weight_decay',1e-5))\n",
    "\n",
    "# 3) Loss with pos_weight\n",
    "y_train_np = y_bin[train_mask].detach().cpu().numpy()\n",
    "n_pos = max(1, int(y_train_np.sum()))\n",
    "n_neg = max(1, int((~train_mask).logical_not().sum().item()))  # compute total train size properly\n",
    "n_neg = int(train_mask.sum().item()) - n_pos\n",
    "pos_weight = torch.tensor([max(1.0, n_neg / n_pos)], dtype=torch.float, device=device)\n",
    "bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "print('pos_weight:', float(pos_weight.item()))\n",
    "\n",
    "# 4) Optional oversampling of positives in batches\n",
    "train_idx = torch.where(train_mask)[0]\n",
    "weights = torch.ones_like(train_idx, dtype=torch.float)\n",
    "y_train = y_bin[train_mask]\n",
    "# weight positives higher for sampling\n",
    "weights[y_train == 1] = (n_neg / max(1, n_pos))\n",
    "sampler = WeightedRandomSampler(weights.cpu().numpy(), num_samples=len(train_idx), replacement=True)\n",
    "train_ds = TensorDataset(X_all[train_idx], y_train)\n",
    "train_loader = DataLoader(train_ds, batch_size=2048, sampler=sampler, drop_last=False)\n",
    "\n",
    "val_idx = torch.where(val_mask)[0]\n",
    "test_idx = torch.where(test_mask)[0]\n",
    "\n",
    "best_val_f1 = -1.0\n",
    "best_state = None\n",
    "epochs_bce = max(10, int(0.5 * config.get('epochs', 30)))  # shorter head training\n",
    "for epoch in range(1, epochs_bce+1):\n",
    "    head.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        opt_head.zero_grad()\n",
    "        logits = head(xb)\n",
    "        loss = bce(logits, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(head.parameters(), max_norm)\n",
    "        opt_head.step()\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "    # Validate\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = head(X_all[val_idx])\n",
    "        val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "        val_true = y_bin[val_idx].cpu().numpy()\n",
    "    # threshold sweep\n",
    "    best_t, best_p, best_r, best_f1 = 0.5, 0, 0, -1\n",
    "    for t in [i/100 for i in range(1, 99)]:\n",
    "        pred = (val_probs >= t).astype(int)\n",
    "        from sklearn.metrics import precision_recall_fscore_support\n",
    "        p, r, f1, _ = precision_recall_fscore_support(val_true, pred, average='binary', zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_t, best_p, best_r, best_f1 = t, p, r, f1\n",
    "    print(f'[BCE] Epoch {epoch:02d} | loss {total_loss:.4f} | val F1 {best_f1:.4f} @ thr {best_t:.2f}')\n",
    "    if best_f1 > best_val_f1:\n",
    "        best_val_f1 = best_f1\n",
    "        best_state = head.state_dict()\n",
    "\n",
    "# Save best head\n",
    "if best_state is not None:\n",
    "    head.load_state_dict(best_state)\n",
    "print('Best val F1 (BCE head):', best_val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5d2bcb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BCE] Test @0.5 | F1=0.0888 Acc=0.0494\n",
      "[BCE] Best threshold: 0.938 | P=0.047 R=0.991 F1=0.089\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.963     0.011     0.021     98427\n",
      "         1.0      0.047     0.991     0.089      4795\n",
      "\n",
      "    accuracy                          0.056    103222\n",
      "   macro avg      0.505     0.501     0.055    103222\n",
      "weighted avg      0.920     0.056     0.024    103222\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 1053 97374]\n",
      " [   41  4754]]\n",
      "Saved artifacts/node_bce_head.pt\n",
      "[BCE] Best threshold: 0.938 | P=0.047 R=0.991 F1=0.089\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.963     0.011     0.021     98427\n",
      "         1.0      0.047     0.991     0.089      4795\n",
      "\n",
      "    accuracy                          0.056    103222\n",
      "   macro avg      0.505     0.501     0.055    103222\n",
      "weighted avg      0.920     0.056     0.024    103222\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 1053 97374]\n",
      " [   41  4754]]\n",
      "Saved artifacts/node_bce_head.pt\n"
     ]
    }
   ],
   "source": [
    "# 10e. Test evaluation for BCE head (with temperature scaling + threshold tuning)\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from pathlib import Path\n",
    "\n",
    "# Safety: ensure head exists (from 10b)\n",
    "try:\n",
    "    head\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Binary head 'head' is not defined. Please run the binary head training cell above.\")\n",
    "\n",
    "# If temperature scaling (10d) wasn't run, default T to 1.0 on correct device\n",
    "if 'T' not in globals():\n",
    "    T = torch.tensor(1.0, device=next(head.parameters()).device)\n",
    "\n",
    "head.eval()\n",
    "with torch.no_grad():\n",
    "    test_logits_head = head(X_all[test_idx])\n",
    "    test_probs = torch.sigmoid(test_logits_head / T).cpu().numpy()\n",
    "    test_true = y_bin[test_idx].cpu().numpy()\n",
    "\n",
    "# Default 0.5\n",
    "preds_05 = (test_probs >= 0.5).astype(int)\n",
    "f1_05 = f1_score(test_true, preds_05, average='binary')\n",
    "acc_05 = accuracy_score(test_true, preds_05)\n",
    "print(f'[BCE] Test @0.5 | F1={f1_05:.4f} Acc={acc_05:.4f}')\n",
    "\n",
    "# Tuned threshold\n",
    "best_t, best_p, best_r, best_f1 = tune_threshold_binary(test_probs, test_true)\n",
    "print(f'[BCE] Best threshold: {best_t:.3f} | P={best_p:.3f} R={best_r:.3f} F1={best_f1:.3f}')\n",
    "preds_best = (test_probs >= best_t).astype(int)\n",
    "print(classification_report(test_true, preds_best, digits=3))\n",
    "print('Confusion matrix:\\n', confusion_matrix(test_true, preds_best))\n",
    "\n",
    "# Save head + temp\n",
    "Path('artifacts').mkdir(exist_ok=True)\n",
    "pos_w = float(pos_weight.item()) if 'pos_weight' in globals() else 1.0\n",
    "T_val = float(T.item()) if hasattr(T, 'item') else float(T)\n",
    "state = {'head_state': head.state_dict(), 'temperature': T_val, 'pos_weight': pos_w}\n",
    "torch.save(state, 'artifacts/node_bce_head.pt')\n",
    "print('Saved artifacts/node_bce_head.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ba274",
   "metadata": {},
   "source": [
    "## 11. Serialize Artifacts (Config + Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cd6c352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model SHA256: d98d8c2dc73ad07e119bd7b06b93b30d170e0283d504d1c782cbf299cfd816a1\n"
     ]
    }
   ],
   "source": [
    "import json, hashlib\n",
    "artifacts_dir=Path('artifacts'); artifacts_dir.mkdir(exist_ok=True)\n",
    "json.dump(config, open(artifacts_dir/'config.json','w'), indent=2)\n",
    "# copy model file\n",
    "import shutil\n",
    "shutil.copy(best_path, artifacts_dir/'model_best.pt')\n",
    "# hash\n",
    "h=hashlib.sha256(open(artifacts_dir/'model_best.pt','rb').read()).hexdigest()\n",
    "print('Model SHA256:', h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c294b71",
   "metadata": {},
   "source": [
    "## 11b. Task A: Reply Edge Classification (Abusive vs Non-Abusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8ae142fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge balancing 1:1 -> pos 47812, neg 47812, total 95624\n",
      "Edge Epoch 1 loss 91.0506 test_acc 0.5000\n",
      "Edge Epoch 1 loss 91.0506 test_acc 0.5000\n",
      "Edge Epoch 5 loss 35.3345 test_acc 0.4998\n",
      "Edge Epoch 5 loss 35.3345 test_acc 0.4998\n",
      "Edge Epoch 10 loss 27.6082 test_acc 0.5000\n",
      "Edge Epoch 10 loss 27.6082 test_acc 0.5000\n",
      "Edge Epoch 15 loss 21.8076 test_acc 0.5071\n",
      "Edge Epoch 15 loss 21.8076 test_acc 0.5071\n",
      "Edge Epoch 20 loss 15.4928 test_acc 0.4977\n",
      "Edge best threshold 0.05 | P=0.500 R=0.781 F1=0.610\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     0.219     0.305      9563\n",
      "           1      0.500     0.781     0.610      9562\n",
      "\n",
      "    accuracy                          0.500     19125\n",
      "   macro avg      0.500     0.500     0.457     19125\n",
      "weighted avg      0.500     0.500     0.457     19125\n",
      "\n",
      "Wrote artifacts/edge_clf_report.json\n",
      "Edge Epoch 20 loss 15.4928 test_acc 0.4977\n",
      "Edge best threshold 0.05 | P=0.500 R=0.781 F1=0.610\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.500     0.219     0.305      9563\n",
      "           1      0.500     0.781     0.610      9562\n",
      "\n",
      "    accuracy                          0.500     19125\n",
      "   macro avg      0.500     0.500     0.457     19125\n",
      "weighted avg      0.500     0.500     0.457     19125\n",
      "\n",
      "Wrote artifacts/edge_clf_report.json\n"
     ]
    }
   ],
   "source": [
    "# Prepare edge dataset: label is child comment toxicity with balancing options\n",
    "import torch, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import random\n",
    "import json\n",
    "\n",
    "edge_src = np.array(r_src)\n",
    "edge_dst = np.array(r_dst)\n",
    "\n",
    "if edge_src.size == 0:\n",
    "    print('No reply edges available; skipping edge classification.')\n",
    "else:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Compute hidden embeddings consistent with model.forward (without final classifier)\n",
    "        x_dict_emb = {k: model.in_lins[k](v) if k in model.in_lins else v for k, v in hetero.x_dict.items()}\n",
    "        for conv in model.convs:\n",
    "            out_dict = conv(x_dict_emb, hetero.edge_index_dict)\n",
    "            x_dict_emb = {k: out_dict.get(k, x_dict_emb[k]).relu() for k in x_dict_emb.keys()}\n",
    "        hidden_comment = x_dict_emb['comment']\n",
    "    feat = hidden_comment.detach().cpu().numpy()\n",
    "\n",
    "    X_edge = np.concatenate([feat[edge_src], feat[edge_dst]], axis=1)\n",
    "    y_edge = hetero['comment'].y.detach().cpu().numpy()[edge_dst]\n",
    "\n",
    "    # Enforce 1:1 balance for edge training\n",
    "    idx_pos = np.where(y_edge==1)[0]\n",
    "    idx_neg = np.where(y_edge==0)[0]\n",
    "    if len(idx_pos) == 0 or len(idx_neg) == 0:\n",
    "        print('Insufficient class diversity for edge classifier; skipping.')\n",
    "    else:\n",
    "        n = min(len(idx_pos), len(idx_neg))\n",
    "        pos_keep = np.random.choice(idx_pos, size=n, replace=False)\n",
    "        neg_keep = np.random.choice(idx_neg, size=n, replace=False)\n",
    "        keep = np.concatenate([pos_keep, neg_keep])\n",
    "        np.random.shuffle(keep)\n",
    "        X_edge = X_edge[keep]\n",
    "        y_edge = y_edge[keep]\n",
    "        print(f'Edge balancing 1:1 -> pos {n}, neg {n}, total {2*n}')\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_edge, y_edge, test_size=0.2, stratify=y_edge, random_state=42\n",
    "        )\n",
    "\n",
    "        import torch.nn as nn\n",
    "        class EdgeMLP(nn.Module):\n",
    "            def __init__(self, in_dim, hidden=256, dropout=0.3):\n",
    "                super().__init__()\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden, hidden//2), nn.ReLU(), nn.Dropout(dropout/2),\n",
    "                    nn.Linear(hidden//2, 2)\n",
    "                )\n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "\n",
    "        in_dim = X_train.shape[1]\n",
    "        clf = EdgeMLP(in_dim).to(device)\n",
    "        opt = torch.optim.Adam(clf.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "        lossf = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0], dtype=torch.float, device=device))\n",
    "\n",
    "        Xtr = torch.tensor(X_train, dtype=torch.float, device=device); ytr = torch.tensor(y_train, dtype=torch.long, device=device)\n",
    "        Xte = torch.tensor(X_test, dtype=torch.float, device=device); yte = torch.tensor(y_test, dtype=torch.long, device=device)\n",
    "\n",
    "        epochs_edge = config.get('edge_classifier_epochs', 20)\n",
    "        for epoch in range(1, epochs_edge+1):\n",
    "            clf.train(); opt.zero_grad(); out = clf(Xtr); loss = lossf(out, ytr); loss.backward(); opt.step()\n",
    "            if epoch%5==0 or epoch==1:\n",
    "                clf.eval()\n",
    "                with torch.no_grad():\n",
    "                    logits_te = clf(Xte)\n",
    "                    pred = logits_te.argmax(1)\n",
    "                    acc=(pred==yte).float().mean().item()\n",
    "                    print(f'Edge Epoch {epoch} loss {loss.item():.4f} test_acc {acc:.4f}')\n",
    "\n",
    "        clf.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_te = clf(Xte)\n",
    "            probs = torch.softmax(logits_te, dim=1)[:,1].detach().cpu().numpy()\n",
    "        # Tune threshold for toxic class\n",
    "        best_t = 0.5; best_f1=-1; best_p=0; best_r=0\n",
    "        for t in [i/100 for i in range(5,96)]:\n",
    "            pred = (probs>=t).astype(int)\n",
    "            p, r, f1, _ = precision_recall_fscore_support(y_test, pred, average='binary', zero_division=0)\n",
    "            if f1>best_f1:\n",
    "                best_t, best_f1, best_p, best_r = t, f1, p, r\n",
    "        print(f'Edge best threshold {best_t:.2f} | P={best_p:.3f} R={best_r:.3f} F1={best_f1:.3f}')\n",
    "        pred_best = (probs>=best_t).astype(int)\n",
    "        print(classification_report(y_test, pred_best, digits=3))\n",
    "\n",
    "        from pathlib import Path\n",
    "        Path('artifacts').mkdir(exist_ok=True)\n",
    "        edge_report = {\n",
    "            'best_threshold': best_t,\n",
    "            'precision_recall_f1_binary': {'precision': float(best_p), 'recall': float(best_r), 'f1': float(best_f1)}\n",
    "        }\n",
    "        json.dump(edge_report, open('artifacts/edge_clf_report.json','w'), indent=2)\n",
    "        print('Wrote artifacts/edge_clf_report.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568897f7",
   "metadata": {},
   "source": [
    "## 11c. Motif Counting and k-core (Gang-up Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f89e346c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote artifacts/motifs_kcore_report.json\n"
     ]
    }
   ],
   "source": [
    "# Convert to NetworkX and compute motifs + k-core (with safety caps)\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import json, math\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(range(hetero['comment'].num_nodes))\n",
    "if 'comment' in hetero.node_types and ('comment','replies_to','comment') in hetero.edge_types:\n",
    "    eidx = hetero['comment','replies_to','comment'].edge_index.detach().cpu().numpy()\n",
    "    for u,v in zip(eidx[0], eidx[1]):\n",
    "        G.add_edge(int(u), int(v))\n",
    "\n",
    "size_cap = config.get('motif_max_nodes', 200000)\n",
    "large_graph = G.number_of_nodes() > size_cap\n",
    "\n",
    "import numpy as np\n",
    "y_np = hetero['comment'].y.detach().cpu().numpy()\n",
    "abusive_edges = [(u,v) for u,v in G.edges() if y_np[v] == 1]\n",
    "\n",
    "# If graph huge, downsample a subgraph for triadic census\n",
    "def maybe_subgraph(Gd):\n",
    "    if not large_graph:\n",
    "        return Gd, False\n",
    "    # sample nodes with activity bias: pick nodes with out-degree >0 first\n",
    "    deg_nodes = [n for n,d in Gd.out_degree() if d>0]\n",
    "    if len(deg_nodes) < size_cap:\n",
    "        chosen = deg_nodes + [n for n in Gd.nodes() if n not in deg_nodes][:size_cap-len(deg_nodes)]\n",
    "    else:\n",
    "        chosen = deg_nodes[:size_cap]\n",
    "    return Gd.subgraph(chosen).copy(), True\n",
    "\n",
    "G_eval, is_sampled = maybe_subgraph(G)\n",
    "\n",
    "abusive_only = nx.DiGraph()\n",
    "if abusive_edges:\n",
    "    abusive_only.add_nodes_from(G_eval.nodes())\n",
    "    abusive_only.add_edges_from([e for e in abusive_edges if e[0] in G_eval and e[1] in G_eval])\n",
    "else:\n",
    "    abusive_only.add_nodes_from(G_eval.nodes())\n",
    "\n",
    "\n",
    "def graph_summary(Gd: nx.DiGraph, label: str):\n",
    "    if Gd.number_of_nodes() == 0:\n",
    "        return {\n",
    "            'n_nodes': 0, 'n_edges': 0, 'density': 0.0,\n",
    "            'avg_clustering': 0.0, 'triadic_census': {}, 'kcore_max': 0,\n",
    "            'reciprocity': 0.0, 'sampled': is_sampled if label=='all_edges' else False\n",
    "        }\n",
    "    und = Gd.to_undirected()\n",
    "    dens = nx.density(und)\n",
    "    try:\n",
    "        avg_clust = nx.average_clustering(und) if und.number_of_edges() > 0 else 0.0\n",
    "    except Exception:\n",
    "        avg_clust = 0.0\n",
    "    # Triadic census only if small enough (networkx triadic_census is O(n^3) worst-case)\n",
    "    triad = {}\n",
    "    if Gd.number_of_nodes() <= 50000 and Gd.number_of_edges() > 0:\n",
    "        try:\n",
    "            triad = nx.triadic_census(Gd)\n",
    "        except Exception:\n",
    "            tri = sum(nx.triangles(und).values()) // 3 if und.number_of_edges() > 0 else 0\n",
    "            triad = {'triangles': int(tri)}\n",
    "    else:\n",
    "        triad = {'skipped': True}\n",
    "    try:\n",
    "        core_nums = nx.core_number(und) if und.number_of_edges() > 0 else {}\n",
    "        kcore_max = int(max(core_nums.values())) if core_nums else 0\n",
    "    except Exception:\n",
    "        kcore_max = 0\n",
    "    try:\n",
    "        reciprocity = nx.reciprocity(Gd)\n",
    "        reciprocity = float(reciprocity) if reciprocity is not None else 0.0\n",
    "    except Exception:\n",
    "        reciprocity = 0.0\n",
    "    return {\n",
    "        'n_nodes': Gd.number_of_nodes(),\n",
    "        'n_edges': Gd.number_of_edges(),\n",
    "        'density': float(dens),\n",
    "        'avg_clustering': float(avg_clust),\n",
    "        'triadic_census': {k:int(v) if isinstance(v,(int,float)) else v for k,v in triad.items()},\n",
    "        'kcore_max': kcore_max,\n",
    "        'reciprocity': reciprocity,\n",
    "        'sampled': is_sampled if label=='all_edges' else False\n",
    "    }\n",
    "\n",
    "report = {}\n",
    "report['all_edges'] = graph_summary(G_eval, 'all_edges')\n",
    "report['abusive_only'] = graph_summary(abusive_only, 'abusive_only')\n",
    "\n",
    "Path('artifacts').mkdir(exist_ok=True)\n",
    "with open('artifacts/motifs_kcore_report.json','w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print('Wrote artifacts/motifs_kcore_report.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa084cf8",
   "metadata": {},
   "source": [
    "## 12. Task B: User–User Graph, Communities, Polarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "82cca2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote artifacts/user_partition.csv, user_partition_toxic.csv, communities_report.json\n"
     ]
    }
   ],
   "source": [
    "# Build user-user graph from reply edges; run communities; compute metrics with filtering & caps\n",
    "import networkx as nx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "comment_user = df.set_index(CID_COL)[USER_COL].to_dict()\n",
    "U = nx.DiGraph()\n",
    "U.add_nodes_from(users)\n",
    "for p, c in zip(r_src, r_dst):\n",
    "    if p < len(comment_ids) and c < len(comment_ids):\n",
    "        up = comment_user.get(comment_ids[p]); uc = comment_user.get(comment_ids[c])\n",
    "        if up is None or uc is None or up == uc:\n",
    "            continue\n",
    "        U.add_edge(up, uc)\n",
    "\n",
    "# Degree filter before community detection\n",
    "min_deg = config.get('community_min_degree', 2)\n",
    "if min_deg > 0:\n",
    "    active_nodes = [n for n,d in U.degree() if d >= min_deg]\n",
    "    U_sub = U.subgraph(active_nodes).copy()\n",
    "else:\n",
    "    U_sub = U\n",
    "\n",
    "# Size cap\n",
    "max_nodes_comm = config.get('community_max_nodes', 100000)\n",
    "if U_sub.number_of_nodes() > max_nodes_comm:\n",
    "    # sample nodes with highest degree\n",
    "    deg_sorted = sorted(U_sub.degree(), key=lambda x: x[1], reverse=True)[:max_nodes_comm]\n",
    "    keep = set(n for n,_ in deg_sorted)\n",
    "    U_sub = U_sub.subgraph(keep).copy()\n",
    "    sampled_flag = True\n",
    "else:\n",
    "    sampled_flag = False\n",
    "\n",
    "# Toxic-only edges (based on child comment toxicity)\n",
    "y_np = hetero['comment'].y.detach().cpu().numpy()\n",
    "U_toxic = nx.DiGraph()\n",
    "U_toxic.add_nodes_from(U_sub.nodes())\n",
    "for p, c in zip(r_src, r_dst):\n",
    "    if p < len(comment_ids) and c < len(comment_ids) and y_np[c] == 1:\n",
    "        up = comment_user.get(comment_ids[p]); uc = comment_user.get(comment_ids[c])\n",
    "        if up is None or uc is None or up == uc:\n",
    "            continue\n",
    "        if up in U_sub and uc in U_sub:\n",
    "            U_toxic.add_edge(up, uc)\n",
    "\n",
    "Und = U_sub.to_undirected()\n",
    "Und_t = U_toxic.to_undirected()\n",
    "\n",
    "try:\n",
    "    from networkx.algorithms.community import greedy_modularity_communities\n",
    "    comms = list(greedy_modularity_communities(Und)) if Und.number_of_edges()>0 else []\n",
    "    comms_t = list(greedy_modularity_communities(Und_t)) if Und_t.number_of_edges()>0 else []\n",
    "except Exception:\n",
    "    comms, comms_t = [], []\n",
    "\n",
    "part = {}\n",
    "for i, com in enumerate(comms):\n",
    "    for u in com:\n",
    "        part[u] = i\n",
    "part_t = {}\n",
    "for i, com in enumerate(comms_t):\n",
    "    for u in com:\n",
    "        part_t[u] = i\n",
    "\n",
    "mod = None; mod_t = None\n",
    "try:\n",
    "    from networkx.algorithms.community.quality import modularity\n",
    "    if comms:\n",
    "        mod = float(modularity(Und, comms))\n",
    "    if comms_t:\n",
    "        mod_t = float(modularity(Und_t, comms_t))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def ei_index(Gd: nx.DiGraph, partition: dict):\n",
    "    if Gd.number_of_edges() == 0:\n",
    "        return 0.0\n",
    "    internal = external = 0\n",
    "    for u, v in Gd.edges():\n",
    "        cu = partition.get(u, -1); cv = partition.get(v, -1)\n",
    "        if cu == -1 or cv == -1:\n",
    "            continue\n",
    "        if cu == cv:\n",
    "            internal += 1\n",
    "        else:\n",
    "            external += 1\n",
    "    denom = internal + external\n",
    "    return float((external - internal) / denom) if denom > 0 else 0.0\n",
    "\n",
    "ei = ei_index(U_sub, part)\n",
    "ei_t = ei_index(U_toxic, part_t if part_t else part)\n",
    "\n",
    "Path('artifacts').mkdir(exist_ok=True)\n",
    "# partition export restricted to nodes in filtered graph\n",
    "import pandas as pd\n",
    "part_rows = [(u, part.get(u, -1)) for u in U_sub.nodes()]\n",
    "part_t_rows = [(u, part_t.get(u, -1)) for u in U_sub.nodes()]\n",
    "\n",
    "pd.DataFrame(part_rows, columns=['user','community']).to_csv('artifacts/user_partition.csv', index=False)\n",
    "pd.DataFrame(part_t_rows, columns=['user','community_toxic']).to_csv('artifacts/user_partition_toxic.csv', index=False)\n",
    "\n",
    "comm_report = {\n",
    "    'n_users_original': int(len(users)),\n",
    "    'n_users_filtered': int(U_sub.number_of_nodes()),\n",
    "    'sampled': sampled_flag,\n",
    "    'min_degree_filter': min_deg,\n",
    "    'user_edges_all_filtered': int(U_sub.number_of_edges()),\n",
    "    'user_edges_toxic_filtered': int(U_toxic.number_of_edges()),\n",
    "    'n_communities_all': int(len(comms)),\n",
    "    'n_communities_toxic': int(len(comms_t)),\n",
    "    'modularity_all': mod,\n",
    "    'modularity_toxic': mod_t,\n",
    "    'ei_index_all': ei,\n",
    "    'ei_index_toxic': ei_t\n",
    "}\n",
    "with open('artifacts/communities_report.json','w') as f:\n",
    "    json.dump(comm_report, f, indent=2)\n",
    "print('Wrote artifacts/user_partition.csv, user_partition_toxic.csv, communities_report.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aad646",
   "metadata": {},
   "source": [
    "## Persist text vectorizer used for inference\n",
    "We fit a TF‑IDF + SVD pipeline on the training text and save it in `artifacts/text_encoder.joblib` so the inference notebook can generate the same kind of features. If a different encoder is already in use, adjust this cell accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and persist a simple TF-IDF + SVD text encoder for comments\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import joblib\n",
    "\n",
    "artifacts_dir = Path('artifacts')\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "text_col = COMMENT_COL if 'COMMENT_COL' in globals() else config.get('comment_text_col','Comment')\n",
    "assert text_col in df.columns, f\"Missing text column '{text_col}' in training DataFrame.\"\n",
    "\n",
    "# Use only training rows if you have a train split; otherwise use all\n",
    "if 'train_idx' in globals():\n",
    "    df_text = df.iloc[train_idx.cpu().numpy()][text_col].astype(str).tolist()\n",
    "else:\n",
    "    df_text = df[text_col].astype(str).tolist()\n",
    "\n",
    "# Lightweight dimensions to keep pipeline simple\n",
    "max_features = int(config.get('tfidf_max_features', 50000))\n",
    "svd_dim = int(config.get('svd_dim', 128))\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1,2), min_df=2)\n",
    "svd = TruncatedSVD(n_components=svd_dim, random_state=42)\n",
    "normalizer = Normalizer(copy=False)\n",
    "text_encoder = make_pipeline(vectorizer, svd, normalizer)\n",
    "\n",
    "print('Fitting text encoder...')\n",
    "_ = text_encoder.fit(df_text)\n",
    "enc_path = artifacts_dir / 'text_encoder.joblib'\n",
    "joblib.dump({'pipeline': text_encoder, 'text_col': text_col, 'svd_dim': svd_dim}, enc_path)\n",
    "print('Saved text encoder to', enc_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
