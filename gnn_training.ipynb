{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ff7f4e",
   "metadata": {},
   "source": [
    "# Toxic Comment Heterogeneous GNN Training Notebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4b46f",
   "metadata": {},
   "source": [
    "## 1. Load Context Configuration (Parse context.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97991e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, os, textwrap\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "CONTEXT_FILE = Path('context.md')\n",
    "assert CONTEXT_FILE.exists(), 'context.md not found in workspace.'\n",
    "raw = CONTEXT_FILE.read_text(encoding='utf-8')\n",
    "\n",
    "# heuristic extraction of CSV path\n",
    "m = re.search(r'youtube_comments_with_toxicity_(\\\\d+_\\\\d+)\\\\.csv', raw)\n",
    "if m:\n",
    "    csv_candidates = list(Path('Notebooks').glob(f'youtube_comments_with_toxicity_{m.group(1)}.csv'))\n",
    "else:\n",
    "    csv_candidates = list(Path('Notebooks').glob('youtube_comments_with_toxicity_*.csv'))\n",
    "DATA_CSV = csv_candidates[0] if csv_candidates else None\n",
    "\n",
    "config = {\n",
    "    'data_csv': str(DATA_CSV) if DATA_CSV else None,\n",
    "    'user_col': 'AuthorChannelID',\n",
    "    'comment_col': 'CommentText',\n",
    "    'comment_id_col': 'CommentID',\n",
    "    'parent_col': 'ParentCommentID',\n",
    "    'label_col': 'ToxicLabel',\n",
    "    'score_col': 'ToxicScore',\n",
    "    'binary_label_col': 'ToxicBinary',\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',\n",
    "    'gnn_hidden': 128,\n",
    "    'gnn_out_classes': 2,\n",
    "    'seed': 42,\n",
    "    'train_val_test_split': [0.8,0.1,0.1],\n",
    "    'primary_metric': 'f1',\n",
    "    'epochs': 30,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'early_stopping_patience': 5,\n",
    "    'batch_size_node_loader': 1024,\n",
    "    'neighbors': 10\n",
    "}\n",
    "print('CONFIG =>')\n",
    "print(json.dumps(config, indent=2))\n",
    "\n",
    "if not DATA_CSV:\n",
    "    raise FileNotFoundError('Could not locate toxicity CSV. Please place it under Notebooks/.')\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "print('Loaded dataframe shape:', df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f0a3c",
   "metadata": {},
   "source": [
    "## 2. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, random\n",
    "from pathlib import Path\n",
    "\n",
    "REQUIRED = ['pandas','numpy','torch','sklearn','tqdm']\n",
    "print('Python version OK')\n",
    "print('Torch:', torch.__version__)\n",
    "\n",
    "def set_seed(seed:int):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(config['seed'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if getattr(torch.backends,'mps',None) and torch.backends.mps.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1cc3b",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cabde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure required columns & create binary label\n",
    "USER_COL=config['user_col']; COMMENT_COL=config['comment_col']; CID_COL=config['comment_id_col']\n",
    "if 'ToxicBinary' not in df.columns:\n",
    "    if df['ToxicLabel'].dtype==object:\n",
    "        df['ToxicBinary']=df['ToxicLabel'].str.lower().str.startswith('toxic').astype(int)\n",
    "    else:\n",
    "        df['ToxicBinary']=(df['ToxicScore']>0.7).astype(int)\n",
    "\n",
    "# Drop rows missing essentials\n",
    "df = df.dropna(subset=[USER_COL, COMMENT_COL])\n",
    "\n",
    "print('Rows after cleaning:', len(df))\n",
    "print('Class balance:', df['ToxicBinary'].value_counts(normalize=True))\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ff63c",
   "metadata": {},
   "source": [
    "## 4. Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model_name = config['embedding_model']\n",
    "    sbert = SentenceTransformer(model_name)\n",
    "    texts = df[COMMENT_COL].astype(str).tolist()\n",
    "    batch=256; embs=[]\n",
    "    for i in range(0,len(texts),batch):\n",
    "        embs.append(sbert.encode(texts[i:i+batch], show_progress_bar=False))\n",
    "    import numpy as np\n",
    "    embeddings = np.vstack(embs)\n",
    "except Exception as e:\n",
    "    print('Falling back to bag-of-words (hash) embeddings due to error:', e)\n",
    "    import numpy as np, hashlib\n",
    "    def hvec(t):\n",
    "        h = hashlib.md5(t.encode()).hexdigest()\n",
    "        return np.array([int(h[i:i+4],16)%10000 for i in range(0,16,4)],dtype=float)\n",
    "    embeddings = np.vstack([hvec(t) for t in df[COMMENT_COL].astype(str)])\n",
    "\n",
    "print('Embeddings shape:', embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a2050",
   "metadata": {},
   "source": [
    "## 5. Build Graph (HeteroData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d7a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "# Reply edges heuristic if parent col missing\n",
    "if config['parent_col'] in df.columns and df[config['parent_col']].notna().any():\n",
    "    reply_pairs = df[df[config['parent_col']].notna()][[config['parent_col'], config['comment_id_col']]].values.tolist()\n",
    "else:\n",
    "    reply_pairs=[]\n",
    "    if 'VideoID' in df.columns and 'PublishedAt' in df.columns:\n",
    "        df_sorted=df.sort_values(['VideoID','PublishedAt'])\n",
    "        for vid, group in df_sorted.groupby('VideoID'):\n",
    "            ids=group[config['comment_id_col']].tolist()\n",
    "            for i in range(1,len(ids)):\n",
    "                reply_pairs.append((ids[i-1], ids[i]))\n",
    "\n",
    "comment_ids = df[CID_COL].astype(str).tolist()\n",
    "comment_idx={cid:i for i,cid in enumerate(comment_ids)}\n",
    "users = df[USER_COL].astype(str).unique().tolist()\n",
    "user_idx={u:i for i,u in enumerate(users)}\n",
    "\n",
    "# user->comment authored edges\n",
    "authored_edges = [(row[USER_COL], row[CID_COL]) for _,row in df.iterrows()]\n",
    "\n",
    "# Build features for users\n",
    "import numpy as np\n",
    "user_deg={u:0 for u in users}; user_tox={u:[] for u in users}\n",
    "for _,r in df.iterrows():\n",
    "    u=r[USER_COL]; user_deg[u]+=1; user_tox[u].append(r['ToxicBinary'])\n",
    "user_feat=np.vstack([\n",
    "    [user_deg[u] for u in users],\n",
    "    [np.mean(user_tox[u]) if user_tox[u] else 0 for u in users]\n",
    "]).T\n",
    "\n",
    "hetero = HeteroData()\n",
    "hetero['comment'].x = torch.tensor(embeddings, dtype=torch.float)\n",
    "hetero['comment'].y = torch.tensor(df['ToxicBinary'].values, dtype=torch.long)\n",
    "hetero['user'].x = torch.tensor(user_feat, dtype=torch.float)\n",
    "\n",
    "src=[user_idx[u] for u,c in authored_edges]\n",
    "dst=[comment_idx[c] for u,c in authored_edges]\n",
    "hetero['user','authored','comment'].edge_index = torch.tensor([src,dst])\n",
    "\n",
    "r_src=[comment_idx[p] for p,c in reply_pairs if p in comment_idx and c in comment_idx]\n",
    "r_dst=[comment_idx[c] for p,c in reply_pairs if p in comment_idx and c in comment_idx]\n",
    "hetero['comment','replies_to','comment'].edge_index = torch.tensor([r_src,r_dst])\n",
    "\n",
    "print(hetero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2348144",
   "metadata": {},
   "source": [
    "## 6. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339257af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "num_comments = hetero['comment'].num_nodes\n",
    "perm = torch.randperm(num_comments)\n",
    "n_train=int(config['train_val_test_split'][0]*num_comments)\n",
    "n_val=int(config['train_val_test_split'][1]*num_comments)\n",
    "train_idx=perm[:n_train]; val_idx=perm[n_train:n_train+n_val]; test_idx=perm[n_train+n_val:]\n",
    "train_mask=torch.zeros(num_comments,dtype=torch.bool); train_mask[train_idx]=True\n",
    "val_mask=torch.zeros(num_comments,dtype=torch.bool); val_mask[val_idx]=True\n",
    "test_mask=torch.zeros(num_comments,dtype=torch.bool); test_mask[test_idx]=True\n",
    "hetero['comment'].train_mask=train_mask\n",
    "hetero['comment'].val_mask=val_mask\n",
    "hetero['comment'].test_mask=test_mask\n",
    "print('Split sizes:', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c1100",
   "metadata": {},
   "source": [
    "## 7. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden, out_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('user','authored','comment'): SAGEConv((-1,-1), hidden),\n",
    "            ('comment','replies_to','comment'): SAGEConv((-1,-1), hidden)\n",
    "        }, aggr='mean')\n",
    "        self.lin = Linear(hidden, out_classes)\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        c = x_dict['comment'].relu()\n",
    "        return self.lin(c)\n",
    "\n",
    "model = HeteroGNN(config['gnn_hidden'], config['gnn_out_classes']).to(device)\n",
    "hetero = hetero.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4cd8f5",
   "metadata": {},
   "source": [
    "## 8. Training Hyperparameters & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "best_metric=-1.0\n",
    "patience=0\n",
    "best_path = Path('model_best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2a130",
   "metadata": {},
   "source": [
    "## 9. Training Loop with Validation & Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4cc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "history = []\n",
    "for epoch in range(1, config['epochs']+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(hetero.x_dict, hetero.edge_index_dict)\n",
    "    y = hetero['comment'].y\n",
    "    loss = criterion(out[hetero['comment'].train_mask], y[hetero['comment'].train_mask])\n",
    "    loss.backward(); optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = out[hetero['comment'].val_mask]\n",
    "        val_y = y[hetero['comment'].val_mask]\n",
    "        preds = val_logits.argmax(dim=1).cpu().numpy()\n",
    "        val_y_np = val_y.cpu().numpy()\n",
    "        f1 = f1_score(val_y_np, preds, average='macro')\n",
    "        acc = accuracy_score(val_y_np, preds)\n",
    "    history.append({'epoch':epoch,'loss':float(loss.item()),'val_f1':float(f1),'val_acc':float(acc)})\n",
    "    print(f\"Epoch {epoch:02d} | loss {loss.item():.4f} | val_f1 {f1:.4f} | val_acc {acc:.4f}\")\n",
    "\n",
    "    if f1>best_metric:\n",
    "        best_metric=f1; patience=0\n",
    "        torch.save({'model_state': model.state_dict(), 'config': config}, best_path)\n",
    "        print('  ✅ Saved new best model (F1={:.4f})'.format(f1))\n",
    "    else:\n",
    "        patience+=1\n",
    "        if patience>=config['early_stopping_patience']:\n",
    "            print('Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "import pandas as _pd\n",
    "hist_df = _pd.DataFrame(history)\n",
    "hist_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08e3b1",
   "metadata": {},
   "source": [
    "## 10. Test Evaluation (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best and evaluate on test\n",
    "state = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(state['model_state'])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(hetero.x_dict, hetero.edge_index_dict)\n",
    "    test_logits = logits[hetero['comment'].test_mask]\n",
    "    test_y = hetero['comment'].y[hetero['comment'].test_mask]\n",
    "    preds = test_logits.argmax(dim=1).cpu().numpy()\n",
    "    y_true = test_y.cpu().numpy()\n",
    "    test_f1 = f1_score(y_true, preds, average='macro')\n",
    "    test_acc = accuracy_score(y_true, preds)\n",
    "print(f'Test F1={test_f1:.4f} Acc={test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ba274",
   "metadata": {},
   "source": [
    "## 11. Serialize Artifacts (Config + Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, hashlib\n",
    "artifacts_dir=Path('artifacts'); artifacts_dir.mkdir(exist_ok=True)\n",
    "json.dump(config, open(artifacts_dir/'config.json','w'), indent=2)\n",
    "# copy model file\n",
    "import shutil\n",
    "shutil.copy(best_path, artifacts_dir/'model_best.pt')\n",
    "# hash\n",
    "h=hashlib.sha256(open(artifacts_dir/'model_best.pt','rb').read()).hexdigest()\n",
    "print('Model SHA256:', h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c294b71",
   "metadata": {},
   "source": [
    "## 11b. Task A: Reply Edge Classification (Abusive vs Non-Abusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae142fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare edge dataset: label is child comment toxicity\n",
    "import torch, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# r_src, r_dst are comment indices for reply edges (built earlier)\n",
    "edge_src = np.array(r_src)\n",
    "edge_dst = np.array(r_dst)\n",
    "\n",
    "if edge_src.size == 0:\n",
    "    print('No reply edges available; skipping edge classification.')\n",
    "else:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_dict = model.conv1(hetero.x_dict, hetero.edge_index_dict)\n",
    "        hidden_comment = hidden_dict['comment']  # tensor [num_comments, hidden]\n",
    "    feat = hidden_comment.detach().cpu().numpy()\n",
    "\n",
    "    # Features: concat (parent_emb || child_emb)\n",
    "    X_edge = np.concatenate([feat[edge_src], feat[edge_dst]], axis=1)\n",
    "    # Labels: abusive if child comment toxic (1), else 0\n",
    "    y_edge = hetero['comment'].y.detach().cpu().numpy()[edge_dst]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_edge, y_edge, test_size=0.2, stratify=y_edge, random_state=42\n",
    "    )\n",
    "\n",
    "    import torch.nn as nn\n",
    "    class EdgeMLP(nn.Module):\n",
    "        def __init__(self, in_dim, hidden=128):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, 2)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    in_dim = X_train.shape[1]\n",
    "    clf = EdgeMLP(in_dim)\n",
    "    opt = torch.optim.Adam(clf.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    lossf = nn.CrossEntropyLoss()\n",
    "\n",
    "    Xtr = torch.tensor(X_train, dtype=torch.float); ytr = torch.tensor(y_train, dtype=torch.long)\n",
    "    Xte = torch.tensor(X_test, dtype=torch.float); yte = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    for epoch in range(1, 21):\n",
    "        clf.train(); opt.zero_grad(); out = clf(Xtr); loss = lossf(out, ytr); loss.backward(); opt.step()\n",
    "        if epoch%5==0:\n",
    "            clf.eval(); pred = clf(Xte).argmax(1); acc=(pred==yte).float().mean().item(); print(f'Epoch {epoch} loss {loss.item():.4f} test_acc {acc:.4f}')\n",
    "\n",
    "    clf.eval(); pred = clf(Xte).argmax(1).numpy()\n",
    "    print(classification_report(y_test, pred, digits=3))\n",
    "\n",
    "    # Save report\n",
    "    import json\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_test, pred, labels=[0,1], zero_division=0)\n",
    "    edge_report = {\n",
    "        'labels': ['non_abusive','abusive'],\n",
    "        'precision': p.tolist(), 'recall': r.tolist(), 'f1': f1.tolist(), 'support': s.tolist()\n",
    "    }\n",
    "    Path('artifacts').mkdir(exist_ok=True)\n",
    "    json.dump(edge_report, open('artifacts/edge_clf_report.json','w'), indent=2)\n",
    "    print('Wrote artifacts/edge_clf_report.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568897f7",
   "metadata": {},
   "source": [
    "## 11c. Motif Counting and k-core (Gang-up Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NetworkX and compute motifs + k-core\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Build a directed graph of comment replies\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(range(hetero['comment'].num_nodes))\n",
    "if 'comment' in hetero.node_types and ('comment','replies_to','comment') in hetero.edge_types:\n",
    "    eidx = hetero['comment','replies_to','comment'].edge_index.detach().cpu().numpy()\n",
    "    for u,v in zip(eidx[0], eidx[1]):\n",
    "        G.add_edge(int(u), int(v))\n",
    "\n",
    "# Edge mask for abusive-only: child comment is toxic\n",
    "import numpy as np\n",
    "abusive_edges = []\n",
    "if G.number_of_edges() > 0:\n",
    "    y_np = hetero['comment'].y.detach().cpu().numpy()\n",
    "    abusive_edges = [(u,v) for u,v in G.edges() if y_np[v] == 1]\n",
    "\n",
    "# Helper to compute summary stats\n",
    "\n",
    "def graph_summary(Gd: nx.DiGraph):\n",
    "    if Gd.number_of_nodes() == 0:\n",
    "        return {\n",
    "            'n_nodes': 0, 'n_edges': 0, 'density': 0.0,\n",
    "            'avg_clustering': 0.0, 'triadic_census': {}, 'kcore_max': 0,\n",
    "            'reciprocity': 0.0\n",
    "        }\n",
    "    und = Gd.to_undirected()\n",
    "    dens = nx.density(und)\n",
    "    try:\n",
    "        avg_clust = nx.average_clustering(und) if und.number_of_edges() > 0 else 0.0\n",
    "    except Exception:\n",
    "        avg_clust = 0.0\n",
    "    try:\n",
    "        triad = nx.triadic_census(Gd)\n",
    "    except Exception:\n",
    "        # Fallback: compute simple count via triangle count on undirected\n",
    "        tri = sum(nx.triangles(und).values()) // 3 if und.number_of_edges() > 0 else 0\n",
    "        triad = {'triangles': int(tri)}\n",
    "    try:\n",
    "        core_nums = nx.core_number(und)\n",
    "        kcore_max = int(max(core_nums.values())) if core_nums else 0\n",
    "    except Exception:\n",
    "        kcore_max = 0\n",
    "    try:\n",
    "        reciprocity = nx.reciprocity(Gd)\n",
    "        reciprocity = float(reciprocity) if reciprocity is not None else 0.0\n",
    "    except Exception:\n",
    "        reciprocity = 0.0\n",
    "    return {\n",
    "        'n_nodes': Gd.number_of_nodes(),\n",
    "        'n_edges': Gd.number_of_edges(),\n",
    "        'density': float(dens),\n",
    "        'avg_clustering': float(avg_clust),\n",
    "        'triadic_census': {k:int(v) for k,v in triad.items()},\n",
    "        'kcore_max': kcore_max,\n",
    "        'reciprocity': reciprocity\n",
    "    }\n",
    "\n",
    "report = {}\n",
    "report['all_edges'] = graph_summary(G)\n",
    "\n",
    "# Abusive-only subgraph\n",
    "if abusive_edges:\n",
    "    G_ab = nx.DiGraph()\n",
    "    G_ab.add_nodes_from(G.nodes())\n",
    "    G_ab.add_edges_from(abusive_edges)\n",
    "    report['abusive_only'] = graph_summary(G_ab)\n",
    "else:\n",
    "    report['abusive_only'] = {\n",
    "        'n_nodes': G.number_of_nodes(), 'n_edges': 0, 'density': 0.0,\n",
    "        'avg_clustering': 0.0, 'triadic_census': {}, 'kcore_max': 0,\n",
    "        'reciprocity': 0.0\n",
    "    }\n",
    "\n",
    "Path('artifacts').mkdir(exist_ok=True)\n",
    "with open('artifacts/motifs_kcore_report.json','w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print('Wrote artifacts/motifs_kcore_report.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa084cf8",
   "metadata": {},
   "source": [
    "## 12. Task B: User–User Graph, Communities, Polarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cca2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build user-user graph from reply edges; run communities; compute metrics\n",
    "import networkx as nx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Map comment -> user\n",
    "comment_user = df.set_index(CID_COL)[USER_COL].to_dict()\n",
    "\n",
    "# Build directed user-user edges (u -> v if u authored parent, v authored child)\n",
    "U = nx.DiGraph()\n",
    "U.add_nodes_from(users)  # users list built earlier\n",
    "for p, c in zip(r_src, r_dst):\n",
    "    # Ensure indices refer to valid comment ids\n",
    "    if p < len(comment_ids) and c < len(comment_ids):\n",
    "        up = comment_user.get(comment_ids[p])\n",
    "        uc = comment_user.get(comment_ids[c])\n",
    "        if up is None or uc is None:\n",
    "            continue\n",
    "        if up == uc:\n",
    "            continue  # skip self-loops\n",
    "        U.add_edge(up, uc)\n",
    "\n",
    "# Toxic-only user edges: child comment is toxic\n",
    "y_np = hetero['comment'].y.detach().cpu().numpy()\n",
    "U_toxic = nx.DiGraph()\n",
    "U_toxic.add_nodes_from(users)\n",
    "for p, c in zip(r_src, r_dst):\n",
    "    if p < len(comment_ids) and c < len(comment_ids) and y_np[c] == 1:\n",
    "        up = comment_user.get(comment_ids[p])\n",
    "        uc = comment_user.get(comment_ids[c])\n",
    "        if up is None or uc is None or up == uc:\n",
    "            continue\n",
    "        U_toxic.add_edge(up, uc)\n",
    "\n",
    "# Community detection (greedy modularity on undirected projection)\n",
    "Und = U.to_undirected()\n",
    "Und_t = U_toxic.to_undirected()\n",
    "\n",
    "try:\n",
    "    from networkx.algorithms.community import greedy_modularity_communities\n",
    "    comms = list(greedy_modularity_communities(Und)) if Und.number_of_edges()>0 else []\n",
    "    comms_t = list(greedy_modularity_communities(Und_t)) if Und_t.number_of_edges()>0 else []\n",
    "except Exception:\n",
    "    comms, comms_t = [], []\n",
    "\n",
    "# Build partition dicts\n",
    "part = {}\n",
    "for i, com in enumerate(comms):\n",
    "    for u in com:\n",
    "        part[u] = i\n",
    "part_t = {}\n",
    "for i, com in enumerate(comms_t):\n",
    "    for u in com:\n",
    "        part_t[u] = i\n",
    "\n",
    "# Modularity (if possible)\n",
    "mod = None\n",
    "mod_t = None\n",
    "try:\n",
    "    from networkx.algorithms.community.quality import modularity\n",
    "    if comms:\n",
    "        mod = float(modularity(Und, comms))\n",
    "    if comms_t:\n",
    "        mod_t = float(modularity(Und_t, comms_t))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# E-I index: for directed graph, compare inter-community edges to intra-community\n",
    "\n",
    "def ei_index(Gd: nx.DiGraph, partition: dict):\n",
    "    if Gd.number_of_edges() == 0:\n",
    "        return 0.0\n",
    "    internal = external = 0\n",
    "    for u, v in Gd.edges():\n",
    "        cu = partition.get(u, -1)\n",
    "        cv = partition.get(v, -1)\n",
    "        if cu == -1 or cv == -1:\n",
    "            continue\n",
    "        if cu == cv:\n",
    "            internal += 1\n",
    "        else:\n",
    "            external += 1\n",
    "    denom = internal + external\n",
    "    return float((external - internal) / denom) if denom > 0 else 0.0\n",
    "\n",
    "ei = ei_index(U, part)\n",
    "ei_t = ei_index(U_toxic, part_t if part_t else part)\n",
    "\n",
    "# Save partitions and metrics\n",
    "Path('artifacts').mkdir(exist_ok=True)\n",
    "part_rows = [(u, part.get(u, -1)) for u in users]\n",
    "part_t_rows = [(u, part_t.get(u, -1)) for u in users]\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(part_rows, columns=['user','community']).to_csv('artifacts/user_partition.csv', index=False)\n",
    "pd.DataFrame(part_t_rows, columns=['user','community_toxic']).to_csv('artifacts/user_partition_toxic.csv', index=False)\n",
    "\n",
    "comm_report = {\n",
    "    'n_users': int(len(users)),\n",
    "    'user_edges_all': int(U.number_of_edges()),\n",
    "    'user_edges_toxic': int(U_toxic.number_of_edges()),\n",
    "    'n_communities_all': int(len(comms)),\n",
    "    'n_communities_toxic': int(len(comms_t)),\n",
    "    'modularity_all': mod,\n",
    "    'modularity_toxic': mod_t,\n",
    "    'ei_index_all': ei,\n",
    "    'ei_index_toxic': ei_t\n",
    "}\n",
    "with open('artifacts/communities_report.json','w') as f:\n",
    "    json.dump(comm_report, f, indent=2)\n",
    "print('Wrote artifacts/user_partition.csv, user_partition_toxic.csv, communities_report.json')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
